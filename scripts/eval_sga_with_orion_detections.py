#!/usr/bin/env python3
"""
Evaluate Scene Graph Anticipation using EXISTING Orion detection pipeline outputs.

This script:
1. Loads detections from Orion's tracks.jsonl (already generated by run_and_eval.py)
2. Maps Orion class names → Action Genome indices
3. Creates detection boxes + features for temporal SGA model
4. Runs inference on trained SGA model
5. Evaluates R@K metrics against Action Genome ground truth

This uses the EXISTING detection pipeline - no new YOLO code!
"""

import os
import sys
import json
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import defaultdict

import numpy as np
import torch
import torch.nn.functional as F

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from orion.sga.temporal_model import TemporalSGAConfig, TemporalSGAModel


# =============================================================================
# ACTION GENOME VOCABULARIES (must match training)
# =============================================================================

OBJECT_CLASSES = [
    '__background__', 'person', 'bag', 'bed', 'blanket', 'book', 'box', 'broom',
    'chair', 'closet/cabinet', 'clothes', 'cup/glass/bottle', 'dish', 'door',
    'doorknob', 'doorway', 'floor', 'food', 'groceries', 'laptop', 'light',
    'medicine', 'mirror', 'paper/notebook', 'phone/camera', 'picture', 'pillow',
    'refrigerator', 'sandwich', 'shelf', 'shoe', 'sofa/couch', 'table',
    'television', 'towel', 'vacuum', 'window'
]
OBJECT_TO_IDX = {cls: i for i, cls in enumerate(OBJECT_CLASSES)}

PREDICATE_CLASSES = [
    '__background__', 'looking_at', 'not_looking_at', 'unsure', 'above',
    'beneath', 'in_front_of', 'behind', 'on_the_side_of', 'in', 'carrying',
    'covered_by', 'drinking_from', 'eating', 'have_it_on_the_back',
    'holding', 'leaning_on', 'lying_on', 'not_contacting', 'other_relationship',
    'sitting_on', 'standing_on', 'touching', 'twisting', 'wearing', 'wiping',
    'writing_on'
]
PREDICATE_TO_IDX = {cls: i for i, cls in enumerate(PREDICATE_CLASSES)}

NUM_OBJECT_CLASSES = len(OBJECT_CLASSES)
NUM_PREDICATE_CLASSES = len(PREDICATE_CLASSES)


# =============================================================================
# ORION → ACTION GENOME CLASS MAPPING
# =============================================================================

# Map Orion detection labels to Action Genome classes
# Orion uses PVSG/COCO vocabulary, AG uses different labels
ORION_TO_AG_MAPPING = {
    # Person variants
    'person': 'person',
    'adult': 'person', 
    'child': 'person',
    'baby': 'person',
    'man': 'person',
    'woman': 'person',
    
    # Furniture
    'chair': 'chair',
    'sofa': 'sofa/couch',
    'couch': 'sofa/couch',
    'bed': 'bed',
    'table': 'table',
    'desk': 'table',
    'dining table': 'table',
    
    # Containers/Storage
    'bag': 'bag',
    'handbag': 'bag',
    'backpack': 'bag',
    'suitcase': 'bag',
    'box': 'box',
    'cabinet': 'closet/cabinet',
    'closet': 'closet/cabinet',
    'cupboard': 'closet/cabinet',
    'refrigerator': 'refrigerator',
    'fridge': 'refrigerator',
    'shelf': 'shelf',
    
    # Household
    'door': 'door',
    'window': 'window',
    'mirror': 'mirror',
    'light': 'light',
    'lamp': 'light',
    
    # Textiles
    'blanket': 'blanket',
    'pillow': 'pillow',
    'towel': 'towel',
    'clothes': 'clothes',
    'shirt': 'clothes',
    'pants': 'clothes',
    
    # Electronics
    'laptop': 'laptop',
    'computer': 'laptop',
    'tv': 'television',
    'television': 'television',
    'phone': 'phone/camera',
    'cellphone': 'phone/camera',
    'camera': 'phone/camera',
    
    # Kitchen/Food
    'cup': 'cup/glass/bottle',
    'glass': 'cup/glass/bottle',
    'bottle': 'cup/glass/bottle',
    'beverage': 'cup/glass/bottle',
    'dish': 'dish',
    'plate': 'dish',
    'bowl': 'dish',
    'food': 'food',
    'sandwich': 'sandwich',
    'cake': 'food',
    'pizza': 'food',
    
    # Other
    'book': 'book',
    'paper': 'paper/notebook',
    'notebook': 'paper/notebook',
    'shoe': 'shoe',
    'broom': 'broom',
    'vacuum': 'vacuum',
    'medicine': 'medicine',
    'groceries': 'groceries',
    'picture': 'picture',
    'floor': 'floor',
    'doorway': 'doorway',
    'doorknob': 'doorknob',
    'countertop': 'table',  # Map countertop to table
    'hat': 'clothes',  # Map hat to clothes
}


def map_orion_to_ag(orion_label: str) -> Optional[int]:
    """Map Orion detection label to Action Genome class index."""
    orion_label_lower = orion_label.lower().strip()
    
    # Direct lookup
    if orion_label_lower in ORION_TO_AG_MAPPING:
        ag_class = ORION_TO_AG_MAPPING[orion_label_lower]
        return OBJECT_TO_IDX.get(ag_class)
    
    # Fuzzy matching - check if any AG class is substring
    for orion_key, ag_class in ORION_TO_AG_MAPPING.items():
        if orion_key in orion_label_lower or orion_label_lower in orion_key:
            return OBJECT_TO_IDX.get(ag_class)
    
    # Check if label directly exists in AG vocabulary
    if orion_label_lower in OBJECT_TO_IDX:
        return OBJECT_TO_IDX[orion_label_lower]
    
    return None


# =============================================================================
# LOAD ORION DETECTIONS
# =============================================================================

def load_orion_detections(tracks_path: Path) -> Dict[int, List[Dict]]:
    """
    Load detections from Orion's tracks.jsonl file.
    
    Returns:
        Dict[frame_id, List[detections]] where each detection has:
        - track_id: int
        - label: str
        - bbox: [x1, y1, x2, y2]
        - confidence: float
    """
    detections_by_frame = defaultdict(list)
    
    with open(tracks_path, 'r') as f:
        for line in f:
            if not line.strip():
                continue
            det = json.loads(line)
            frame_id = det['frame_id']
            detections_by_frame[frame_id].append({
                'track_id': det['track_id'],
                'label': det['label'],
                'bbox': det['bbox'],
                'confidence': det.get('confidence', 1.0)
            })
    
    return dict(detections_by_frame)


def orion_detections_to_ag_format(
    detections_by_frame: Dict[int, List[Dict]],
    frame_ids: List[int],
    max_objects: int = 20
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Convert Orion detections to Action Genome format tensors.
    
    Returns:
        boxes: [T, max_objects, 4] - bounding boxes
        labels: [T, max_objects] - class indices (0 = padding)
        masks: [T, max_objects] - 1 where valid, 0 for padding
    """
    T = len(frame_ids)
    
    boxes = torch.zeros(T, max_objects, 4, dtype=torch.float32)
    labels = torch.zeros(T, max_objects, dtype=torch.long)
    masks = torch.zeros(T, max_objects, dtype=torch.float32)
    
    for t, frame_id in enumerate(frame_ids):
        frame_dets = detections_by_frame.get(frame_id, [])
        
        n_valid = 0
        for det in frame_dets:
            if n_valid >= max_objects:
                break
            
            # Map label to AG class
            ag_idx = map_orion_to_ag(det['label'])
            if ag_idx is None or ag_idx == 0:  # Skip background or unmapped
                continue
            
            # Store detection
            boxes[t, n_valid] = torch.tensor(det['bbox'][:4], dtype=torch.float32)
            labels[t, n_valid] = ag_idx
            masks[t, n_valid] = 1.0
            n_valid += 1
    
    return boxes, labels, masks


# =============================================================================
# ACTION GENOME GROUND TRUTH LOADER
# =============================================================================

def load_ag_ground_truth(ag_annotation_dir: Path, video_id: str) -> Dict[int, List[Tuple[int, int, int]]]:
    """
    Load Action Genome ground truth triplets for a video.
    
    Returns:
        Dict[frame_id, List[(subject_idx, predicate_idx, object_idx)]]
    """
    # AG annotations are in pickle format
    frame_list_path = ag_annotation_dir / "frame_list.pkl"
    annotations_path = ag_annotation_dir / "annotations" / f"{video_id}.pkl"
    
    if not annotations_path.exists():
        return {}
    
    with open(annotations_path, 'rb') as f:
        video_annotations = pickle.load(f)
    
    gt_by_frame = defaultdict(list)
    
    for frame_data in video_annotations:
        frame_id = frame_data.get('frame_id', 0)
        
        for triplet in frame_data.get('triplets', []):
            subj_class = triplet.get('subject', {}).get('class')
            obj_class = triplet.get('object', {}).get('class')
            predicate = triplet.get('predicate')
            
            if subj_class and obj_class and predicate:
                subj_idx = OBJECT_TO_IDX.get(subj_class, 0)
                obj_idx = OBJECT_TO_IDX.get(obj_class, 0)
                pred_idx = PREDICATE_TO_IDX.get(predicate, 0)
                
                if subj_idx > 0 and obj_idx > 0 and pred_idx > 0:
                    gt_by_frame[frame_id].append((subj_idx, pred_idx, obj_idx))
    
    return dict(gt_by_frame)


# =============================================================================
# SGA EVALUATION
# =============================================================================

def compute_recall_at_k(
    predictions: List[Tuple[int, int, int, float]],  # (subj, pred, obj, score)
    ground_truth: List[Tuple[int, int, int]],  # (subj, pred, obj)
    k: int
) -> float:
    """Compute Recall@K for predicted vs ground truth triplets."""
    if not ground_truth:
        return 0.0
    
    # Sort predictions by score descending
    sorted_preds = sorted(predictions, key=lambda x: x[3], reverse=True)[:k]
    
    # Convert to set of (subj, pred, obj) for matching
    pred_triplets = set((p[0], p[1], p[2]) for p in sorted_preds)
    gt_triplets = set(ground_truth)
    
    # Count matches
    matches = len(pred_triplets & gt_triplets)
    recall = matches / len(gt_triplets)
    
    return recall


class SGAEvaluator:
    """Evaluate Scene Graph Anticipation with Orion detections."""
    
    def __init__(self, model_path: str, device: str = "mps"):
        self.device = device
        
        # Load trained model
        print(f"Loading SGA model from {model_path}...")
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        
        config = checkpoint.get('config', TemporalSGAConfig())
        if isinstance(config, dict):
            config = TemporalSGAConfig(**config)
        
        self.model = TemporalSGAModel(config)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(device)
        self.model.eval()
        
        self.config = config
        print(f"✓ Model loaded ({sum(p.numel() for p in self.model.parameters()):,} parameters)")
    
    def predict_future_scene_graph(
        self,
        boxes: torch.Tensor,  # [T, N, 4]
        labels: torch.Tensor,  # [T, N]
        masks: torch.Tensor,  # [T, N]
        future_steps: int = 1
    ) -> List[List[Tuple[int, int, int, float]]]:
        """
        Predict scene graph at future time steps.
        
        Returns:
            List of predicted triplets for each future step
        """
        with torch.no_grad():
            # Add batch dimension
            boxes = boxes.unsqueeze(0).to(self.device)
            labels = labels.unsqueeze(0).to(self.device)
            masks = masks.unsqueeze(0).to(self.device)
            
            # Forward through model
            outputs = self.model(boxes, labels, masks)
            
            # Extract predictions for future steps
            future_triplets = outputs.get('future_triplets', [])
            
            all_predictions = []
            for step_outputs in future_triplets[:future_steps]:
                step_preds = []
                for pred in step_outputs:
                    # pred format: (future_frame_idx, subj_idx, pred_idx, obj_idx, score)
                    subj = pred[1] if len(pred) > 1 else 0
                    predicate = pred[2] if len(pred) > 2 else 0
                    obj = pred[3] if len(pred) > 3 else 0
                    score = pred[4] if len(pred) > 4 else 1.0
                    step_preds.append((subj, predicate, obj, score))
                all_predictions.append(step_preds)
            
            return all_predictions
    
    def evaluate_video(
        self,
        tracks_path: Path,
        gt_triplets: Dict[int, List[Tuple[int, int, int]]],
        observe_frames: int = 8,
        anticipate_frames: int = 3
    ) -> Dict[str, float]:
        """
        Evaluate SGA on a single video using Orion detections.
        
        Args:
            tracks_path: Path to Orion tracks.jsonl
            gt_triplets: Ground truth triplets by frame
            observe_frames: Number of frames to observe
            anticipate_frames: Number of future frames to predict
        
        Returns:
            Dict with R@10, R@20, R@50 metrics
        """
        # Load Orion detections
        detections = load_orion_detections(tracks_path)
        
        if not detections:
            print(f"  No detections found in {tracks_path}")
            return {'R@10': 0, 'R@20': 0, 'R@50': 0, 'num_samples': 0}
        
        # Get sorted frame IDs
        frame_ids = sorted(detections.keys())
        
        if len(frame_ids) < observe_frames + anticipate_frames:
            print(f"  Not enough frames: {len(frame_ids)} < {observe_frames + anticipate_frames}")
            return {'R@10': 0, 'R@20': 0, 'R@50': 0, 'num_samples': 0}
        
        # Evaluate at multiple time points
        r10_scores, r20_scores, r50_scores = [], [], []
        
        # Sample evaluation points
        num_samples = min(10, len(frame_ids) - observe_frames - anticipate_frames)
        sample_indices = np.linspace(0, len(frame_ids) - observe_frames - anticipate_frames - 1, num_samples, dtype=int)
        
        for start_idx in sample_indices:
            # Get observation window
            obs_frame_ids = frame_ids[start_idx:start_idx + observe_frames]
            future_frame_ids = frame_ids[start_idx + observe_frames:start_idx + observe_frames + anticipate_frames]
            
            # Convert to tensors
            boxes, labels, masks = orion_detections_to_ag_format(
                detections, obs_frame_ids, max_objects=20
            )
            
            # Skip if too few valid detections
            if masks.sum() < observe_frames * 2:
                continue
            
            # Get predictions
            try:
                predictions = self.predict_future_scene_graph(boxes, labels, masks, anticipate_frames)
            except Exception as e:
                print(f"  Prediction failed: {e}")
                continue
            
            # Evaluate against ground truth for each future frame
            for i, future_frame_id in enumerate(future_frame_ids):
                if future_frame_id not in gt_triplets:
                    continue
                
                gt = gt_triplets[future_frame_id]
                if not gt:
                    continue
                
                preds = predictions[i] if i < len(predictions) else []
                
                r10_scores.append(compute_recall_at_k(preds, gt, k=10))
                r20_scores.append(compute_recall_at_k(preds, gt, k=20))
                r50_scores.append(compute_recall_at_k(preds, gt, k=50))
        
        if not r20_scores:
            return {'R@10': 0, 'R@20': 0, 'R@50': 0, 'num_samples': 0}
        
        return {
            'R@10': np.mean(r10_scores) * 100,
            'R@20': np.mean(r20_scores) * 100,
            'R@50': np.mean(r50_scores) * 100,
            'num_samples': len(r20_scores)
        }


# =============================================================================
# MAIN
# =============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate SGA with Orion detections")
    parser.add_argument("--model", type=str, default="models/temporal_sga_best.pt",
                       help="Path to trained SGA model")
    parser.add_argument("--results-dir", type=str, default="results",
                       help="Directory with Orion detection results")
    parser.add_argument("--ag-dir", type=str, default="data/ag",
                       help="Action Genome annotation directory")
    parser.add_argument("--video-id", type=str, default=None,
                       help="Specific video ID to evaluate (optional)")
    parser.add_argument("--max-videos", type=int, default=10,
                       help="Maximum number of videos to evaluate")
    parser.add_argument("--device", type=str, default="mps",
                       choices=["mps", "cuda", "cpu"])
    args = parser.parse_args()
    
    # Initialize evaluator
    evaluator = SGAEvaluator(args.model, args.device)
    
    results_dir = Path(args.results_dir)
    
    # Find videos with Orion detections
    if args.video_id:
        video_dirs = [results_dir / args.video_id]
    else:
        video_dirs = [d for d in results_dir.iterdir() 
                     if d.is_dir() and (d / "tracks.jsonl").exists()]
    
    video_dirs = video_dirs[:args.max_videos]
    
    print(f"\n{'='*60}")
    print(f"Evaluating SGA with Orion Detections")
    print(f"{'='*60}")
    print(f"Model: {args.model}")
    print(f"Results dir: {args.results_dir}")
    print(f"Videos found: {len(video_dirs)}")
    print(f"{'='*60}\n")
    
    all_metrics = []
    
    for video_dir in video_dirs:
        video_id = video_dir.name
        tracks_path = video_dir / "tracks.jsonl"
        
        if not tracks_path.exists():
            continue
        
        # Check file has content
        if tracks_path.stat().st_size == 0:
            continue
        
        print(f"\nEvaluating: {video_id}")
        
        # Load ground truth (if available)
        # For now, use placeholder GT since we need AG→Orion video mapping
        gt_triplets = {}  # Would need video mapping to load real GT
        
        metrics = evaluator.evaluate_video(
            tracks_path,
            gt_triplets,
            observe_frames=8,
            anticipate_frames=3
        )
        
        print(f"  Detections loaded, {metrics.get('num_samples', 0)} samples")
        
        if metrics['num_samples'] > 0:
            print(f"  R@10: {metrics['R@10']:.2f}%")
            print(f"  R@20: {metrics['R@20']:.2f}%")
            print(f"  R@50: {metrics['R@50']:.2f}%")
            all_metrics.append(metrics)
    
    # Summary
    if all_metrics:
        print(f"\n{'='*60}")
        print(f"SUMMARY ({len(all_metrics)} videos)")
        print(f"{'='*60}")
        
        avg_r10 = np.mean([m['R@10'] for m in all_metrics])
        avg_r20 = np.mean([m['R@20'] for m in all_metrics])
        avg_r50 = np.mean([m['R@50'] for m in all_metrics])
        
        print(f"Mean R@10: {avg_r10:.2f}%")
        print(f"Mean R@20: {avg_r20:.2f}%")
        print(f"Mean R@50: {avg_r50:.2f}%")
    else:
        print("\nNo valid evaluation samples found.")
        print("Note: Need video ID mapping between Orion results and AG ground truth.")


if __name__ == "__main__":
    main()
