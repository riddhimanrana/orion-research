{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b8ed71",
   "metadata": {},
   "source": [
    "# DINOv3 + SAM + VO Evaluation (Accuracy-First)\n",
    "\n",
    "This notebook evaluates DINOv3 embeddings against CLIP for egocentric Re-ID, adds optional SAM masking, explores dense feature heatmaps and basic VO (pose) baselines, and saves visual outputs for medium-term impact assessment before real-time tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7dca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:46:00) [Clang 18.1.8 ]\n",
      "Installed: {\n",
      "  \"torch\": \"2.5.1\",\n",
      "  \"torchvision\": \"0.20.1\",\n",
      "  \"timm\": \"1.0.22\",\n",
      "  \"transformers\": \"4.57.1\",\n",
      "  \"accelerate\": \"1.6.0\",\n",
      "  \"opencv-python\": null,\n",
      "  \"scikit-learn\": null,\n",
      "  \"matplotlib\": \"3.10.6\",\n",
      "  \"einops\": \"0.6.1\",\n",
      "  \"numpy\": \"1.26.4\"\n",
      "}\n",
      "Missing packages: ['opencv-python', 'scikit-learn']\n",
      "Tip (zsh): pip install -U  opencv-python scikit-learn\n",
      "Torch version: 2.5.1\n",
      "CUDA available: False\n",
      "MPS available: False\n"
     ]
    }
   ],
   "source": [
    "# 1) Environment Setup and Version Pinning\n",
    "import sys, os, subprocess, json\n",
    "from pathlib import Path\n",
    "\n",
    "required = {\n",
    "    \"torch\": \"2.2\",\n",
    "    \"torchvision\": None,\n",
    "    \"timm\": \"1.0.20\",\n",
    "    \"transformers\": \"4.56.0\",\n",
    "    \"accelerate\": None,\n",
    "    \"opencv-python\": None,\n",
    "    \"scikit-learn\": None,\n",
    "    \"matplotlib\": None,\n",
    "    \"einops\": None,\n",
    "    \"numpy\": None,\n",
    "}\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "# Print installed versions and device info\n",
    "missing = []\n",
    "versions = {}\n",
    "for pkg, ver in required.items():\n",
    "    try:\n",
    "        mod = __import__(pkg.replace(\"-\", \"_\"))\n",
    "        v = getattr(mod, \"__version__\", \"unknown\")\n",
    "        versions[pkg] = v\n",
    "    except Exception:\n",
    "        versions[pkg] = None\n",
    "        missing.append(pkg)\n",
    "\n",
    "print(\"Installed:\", json.dumps(versions, indent=2))\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Tip (zsh): pip install -U \", \" \".join([f\"{m}{'>='+required[m] if required[m] else ''}\" for m in missing]))\n",
    "\n",
    "# CUDA/MPS info\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"MPS available:\", getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available())\n",
    "except Exception as e:\n",
    "    print(\"Torch not importable:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349f16ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 2) GPU/Precision Configuration and Reproducibility\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "CFG = {\n",
    "    \"precision\": \"fp32\",  # fp32|bf16|fp16\n",
    "    \"deterministic\": True,\n",
    "    \"compile\": False,\n",
    "    \"batch_size\": 8,\n",
    "    \"img_size\": 256,\n",
    "}\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "try:\n",
    "    import torch\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    if CFG[\"deterministic\"]:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    if CFG[\"precision\"] in (\"bf16\", \"fp16\") and torch.cuda.is_available():\n",
    "        dtype = torch.bfloat16 if CFG[\"precision\"] == \"bf16\" else torch.float16\n",
    "    else:\n",
    "        dtype = torch.float32\n",
    "    print(\"Using dtype:\", dtype)\n",
    "except Exception as e:\n",
    "    print(\"Torch config skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bbb8c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 images from /Users/riddhiman.rana/Desktop/Coding/Orion/orion-research/data/examples\n"
     ]
    }
   ],
   "source": [
    "# 3) Load Images and Video Frames\n",
    "from typing import List, Tuple\n",
    "import cv2\n",
    "\n",
    "DATA_ROOT = Path(\"../data/examples\").resolve()\n",
    "OUTPUT_DIR = Path(\"../results/dino_eval_outputs\").resolve()\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_images_from_dir(img_dir: Path, max_images: int = 64) -> List[np.ndarray]:\n",
    "    imgs = []\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "    for p in sorted(img_dir.glob(\"**/*\")):\n",
    "        if p.suffix.lower() in exts:\n",
    "            bgr = cv2.imread(str(p))\n",
    "            if bgr is None:\n",
    "                continue\n",
    "            rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "            imgs.append(rgb)\n",
    "            if len(imgs) >= max_images:\n",
    "                break\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def sample_video_frames(video_path: Path, skip: int = 10, max_frames: int = 200) -> List[np.ndarray]:\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    frames = []\n",
    "    i = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if i % skip == 0:\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(rgb)\n",
    "            if len(frames) >= max_frames:\n",
    "                break\n",
    "        i += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Demo load\n",
    "images = load_images_from_dir(DATA_ROOT, max_images=32)\n",
    "print(f\"Loaded {len(images)} images from {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63dd472f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transformers load failed (You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/facebook/dinov3-vitb16-pretrain-lvd1689m.\n",
      "403 Client Error. (Request ID: Root=1-691543ab-30a49c832414567178f0870a;11e2e9e2-ba9c-4f0f-97e8-dd8455351b8f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/facebook/dinov3-vitb16-pretrain-lvd1689m/resolve/main/preprocessor_config.json.\n",
      "Access to model facebook/dinov3-vitb16-pretrain-lvd1689m is restricted and you are not in the authorized list. Visit https://huggingface.co/facebook/dinov3-vitb16-pretrain-lvd1689m to ask for access.); falling back to timm…\n",
      "Failed to load DINO: Could not load DINO. Install either 'transformers>=4.56.0' or 'timm>=1.0.20'.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back: direct Transformers/timm loading failed: Could not load DINO. Install either 'transformers>=4.56.0' or 'timm>=1.0.20'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP backend ready via ModelManager\n"
     ]
    }
   ],
   "source": [
    "# 4) DINOv3 Backbones via Transformers and timm\n",
    "from orion.managers.model_manager import ModelManager\n",
    "\n",
    "mm = ModelManager.get_instance()\n",
    "\n",
    "# Prefer using our backend wrapper for consistency across codebase\n",
    "try:\n",
    "    dino = mm.dino\n",
    "    print(\"DINO backend ready via ModelManager\")\n",
    "except Exception as e:\n",
    "    print(\"Falling back: direct Transformers/timm loading failed:\", e)\n",
    "    dino = None\n",
    "\n",
    "# Also expose CLIP for comparison (label verification baseline)\n",
    "try:\n",
    "    clip = mm.clip\n",
    "    print(\"CLIP backend ready via ModelManager\")\n",
    "except Exception as e:\n",
    "    print(\"CLIP unavailable:\", e)\n",
    "    clip = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Preprocessing Transforms (LVD-1689M vs SAT-493M)\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "mean_lvd = (0.485, 0.456, 0.406)\n",
    "std_lvd  = (0.229, 0.224, 0.225)\n",
    "\n",
    "mean_sat = (0.430, 0.411, 0.296)\n",
    "std_sat  = (0.213, 0.156, 0.143)\n",
    "\n",
    "def make_transform(img_size: int = 256, sat: bool = False):\n",
    "    m = mean_sat if sat else mean_lvd\n",
    "    s = std_sat if sat else std_lvd\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((img_size, img_size), antialias=True),\n",
    "        transforms.ConvertImageDtype(dtype=torch.float32),\n",
    "        transforms.Normalize(mean=m, std=s),\n",
    "    ])\n",
    "\n",
    "# Smoke test\n",
    "if images:\n",
    "    t = make_transform(CFG[\"img_size\"], sat=False)\n",
    "    test_tensor = t(images[0])\n",
    "    assert test_tensor.shape[1] == CFG[\"img_size\"], \"Transform resize failed\"\n",
    "    print(\"Transform OK:\", tuple(test_tensor.shape))\n",
    "else:\n",
    "    print(\"No images loaded yet; skipping transform test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f286eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Dense Feature Extraction and Pooling\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def l2n(v: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
    "    n = np.linalg.norm(v) + eps\n",
    "    return (v / n).astype(np.float32)\n",
    "\n",
    "\n",
    "def encode_images(embedder, imgs: list) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for im in imgs:\n",
    "        try:\n",
    "            v = embedder.encode_image(im)\n",
    "            vecs.append(v)\n",
    "        except Exception as e:\n",
    "            # Fallback: skip image\n",
    "            print(\"Embedding failed:\", e)\n",
    "    if not vecs:\n",
    "        return np.zeros((0, 1), dtype=np.float32)\n",
    "    return np.stack(vecs, axis=0)\n",
    "\n",
    "\n",
    "clip_vecs = encode_images(clip, images) if clip else None\n",
    "dino_vecs = encode_images(dino, images) if dino else None\n",
    "\n",
    "print(\n",
    "    \"Shapes:\",\n",
    "    {\"clip\": None if clip_vecs is None else clip_vecs.shape,\n",
    "     \"dino\": None if dino_vecs is None else dino_vecs.shape}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d49733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Cosine Similarity Heatmaps and Token Matching\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def cosine_sim_matrix(X: np.ndarray) -> np.ndarray:\n",
    "    if X is None or len(X) == 0:\n",
    "        return np.zeros((0, 0), dtype=np.float32)\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-8)\n",
    "    return Xn @ Xn.T\n",
    "\n",
    "\n",
    "if dino_vecs is not None and len(dino_vecs) >= 2:\n",
    "    S = cosine_sim_matrix(dino_vecs)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(S, vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"DINO global cosine similarity\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"dino_global_similarity.png\", dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough DINO vectors for similarity plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96854294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Segmentation with SAM (point/box prompts) — optional\n",
    "try:\n",
    "    from transformers import SamModel, SamProcessor\n",
    "    _sam_available = True\n",
    "    print(\"Transformers SAM available\")\n",
    "except Exception:\n",
    "    _sam_available = False\n",
    "    print(\"SAM not available; skipping segmentation\")\n",
    "\n",
    "sam_model = None\n",
    "sam_processor = None\n",
    "\n",
    "if _sam_available:\n",
    "    try:\n",
    "        sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "        sam_model = SamModel.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "        if torch.cuda.is_available():\n",
    "            sam_model = sam_model.to(\"cuda\")\n",
    "        elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "            sam_model = sam_model.to(\"mps\")\n",
    "        else:\n",
    "            sam_model = sam_model.to(\"cpu\")\n",
    "        print(\"SAM model loaded\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load SAM:\", e)\n",
    "        _sam_available = False\n",
    "\n",
    "\n",
    "def run_sam_masks(image_rgb: np.ndarray, points=None, boxes=None):\n",
    "    if not _sam_available or sam_model is None or sam_processor is None:\n",
    "        return []\n",
    "    inputs = sam_processor(images=image_rgb, input_points=points, input_boxes=boxes, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(sam_model.device) for k, v in inputs.items()}\n",
    "    with torch.inference_mode():\n",
    "        outputs = sam_model(**inputs)\n",
    "    # Basic thresholding\n",
    "    masks = outputs.pred_masks.squeeze(0).detach().cpu().numpy()  # [N, H, W]\n",
    "    bin_masks = [(m > 0.0).astype(np.uint8) for m in masks]\n",
    "    return bin_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad306251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Mask-Guided Feature Pooling and Object Descriptors\n",
    "\n",
    "def masked_mean_descriptor(embedder, image_rgb: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    # Fallback to global if token features aren't exposed by backend; use crop\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return embedder.encode_image(image_rgb)\n",
    "    x0, x1 = int(xs.min()), int(xs.max())\n",
    "    y0, y1 = int(ys.min()), int(ys.max())\n",
    "    crop = image_rgb[y0:y1+1, x0:x1+1]\n",
    "    return embedder.encode_image(crop)\n",
    "\n",
    "obj_descs = []\n",
    "if dino is not None and images:\n",
    "    if _sam_available:\n",
    "        ms = run_sam_masks(images[0])\n",
    "        print(f\"SAM masks: {len(ms)}\")\n",
    "        for m in ms[:5]:\n",
    "            obj_descs.append(masked_mean_descriptor(dino, images[0], m))\n",
    "    else:\n",
    "        # Create a simple box mask as placeholder\n",
    "        h, w, _ = images[0].shape\n",
    "        bx = int(0.2 * w), int(0.2 * h), int(0.6 * w), int(0.6 * h)\n",
    "        m = np.zeros((h, w), dtype=np.uint8)\n",
    "        m[bx[1]:bx[3], bx[0]:bx[2]] = 1\n",
    "        obj_descs.append(masked_mean_descriptor(dino, images[0], m))\n",
    "\n",
    "print(\"Object descriptors:\", None if not obj_descs else np.stack(obj_descs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Frame-to-Frame Matching and Pose Estimation (VO baseline)\n",
    "from typing import Optional\n",
    "\n",
    "def sample_keypoints(image_rgb: np.ndarray, n: int = 200) -> np.ndarray:\n",
    "    # Use a simple grid sampler as placeholder\n",
    "    h, w, _ = image_rgb.shape\n",
    "    ys = np.linspace(h*0.1, h*0.9, int(np.sqrt(n))).astype(int)\n",
    "    xs = np.linspace(w*0.1, w*0.9, int(np.sqrt(n))).astype(int)\n",
    "    pts = np.array([(x, y) for y in ys for x in xs], dtype=np.float32)\n",
    "    return pts[:n]\n",
    "\n",
    "\n",
    "def match_points(p1: np.ndarray, p2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Identity correspondence for placeholder grid\n",
    "    n = min(len(p1), len(p2))\n",
    "    return p1[:n], p2[:n]\n",
    "\n",
    "\n",
    "def estimate_pose(p1: np.ndarray, p2: np.ndarray, K: Optional[np.ndarray] = None):\n",
    "    if K is None:\n",
    "        # Focal ~ 1000 px default placeholder, principal center\n",
    "        K = np.array([[1000.0, 0, 640.0], [0, 1000.0, 360.0], [0, 0, 1]])\n",
    "    E, mask = cv2.findEssentialMat(p1, p2, K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "    if E is None:\n",
    "        return None, None, None\n",
    "    _, R, t, mask2 = cv2.recoverPose(E, p1, p2, K)\n",
    "    return R, t.squeeze(), (mask, mask2)\n",
    "\n",
    "if len(images) >= 2:\n",
    "    pts1 = sample_keypoints(images[0])\n",
    "    pts2 = sample_keypoints(images[1])\n",
    "    m1, m2 = match_points(pts1, pts2)\n",
    "    R, t, m = estimate_pose(m1, m2)\n",
    "    print(\"Pose R:\\n\", R)\n",
    "    print(\"Pose t:\", t)\n",
    "else:\n",
    "    print(\"Need at least 2 frames for VO baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f219cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Minimal Trajectory Integration and Visualization\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "def integrate_trajectory(R_list, t_list):\n",
    "    # Simple chaining in camera coordinates\n",
    "    poses = []\n",
    "    T = np.eye(4)\n",
    "    poses.append(T.copy())\n",
    "    for R, t in zip(R_list, t_list):\n",
    "        Ti = np.eye(4)\n",
    "        Ti[:3, :3] = R\n",
    "        Ti[:3, 3] = t\n",
    "        T = T @ Ti\n",
    "        poses.append(T.copy())\n",
    "    return poses\n",
    "\n",
    "# Placeholder if we estimated only one relative pose\n",
    "R_list = []\n",
    "t_list = []\n",
    "if len(images) >= 3:\n",
    "    for i in range(2):\n",
    "        p1 = sample_keypoints(images[i])\n",
    "        p2 = sample_keypoints(images[i+1])\n",
    "        m1, m2 = match_points(p1, p2)\n",
    "        R, t, _ = estimate_pose(m1, m2)\n",
    "        if R is not None:\n",
    "            R_list.append(R)\n",
    "            t_list.append(t)\n",
    "\n",
    "poses = integrate_trajectory(R_list, t_list)\n",
    "\n",
    "# Plot 2D and 3D\n",
    "xyz = np.array([P[:3, 3] for P in poses]) if poses else np.zeros((0, 3))\n",
    "plt.figure(figsize=(5,4))\n",
    "if len(xyz):\n",
    "    plt.plot(xyz[:,0], xyz[:,2], marker='o')\n",
    "    plt.title(\"Trajectory XZ\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Z\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR/\"trajectory_xz.png\", dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No trajectory to plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Optional: Depth Prediction via DINOv3 Depther Head (skip if unavailable)\n",
    "try:\n",
    "    import torch\n",
    "    REPO_DIR = \".\"  # requires local dinov3 repo; leave as placeholder\n",
    "    # depther = torch.hub.load(REPO_DIR, 'dinov3_vit7b16_dd', source='local', weights='PATH', backbone_weights='PATH')\n",
    "    print(\"Depther example requires local weights; skipping by default\")\n",
    "except Exception as e:\n",
    "    print(\"Depther not available:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) k-NN Probe on Small Labeled Subset (proxy for representation)\n",
    "try:\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    _sk_ok = True\n",
    "except Exception:\n",
    "    _sk_ok = False\n",
    "\n",
    "# Minimal synthetic labels: alternate labels for demo if no dataset\n",
    "if dino_vecs is not None and len(dino_vecs) >= 10 and _sk_ok:\n",
    "    X = dino_vecs\n",
    "    y = np.array([(i % 2) for i in range(len(X))])\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=SEED, stratify=y)\n",
    "    clf = KNeighborsClassifier(n_neighbors=5, metric='cosine')\n",
    "    clf.fit(Xtr, ytr)\n",
    "    pred = clf.predict(Xte)\n",
    "    acc = accuracy_score(yte, pred)\n",
    "    print(f\"k-NN proxy (binary alt labels) accuracy: {acc:.3f}\")\n",
    "else:\n",
    "    print(\"Skipping k-NN probe (need sklearn and >=10 embeddings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Performance Profiling and Memory Tracking\n",
    "import time\n",
    "\n",
    "def profile_embedder(embedder, imgs, n_warmup=2):\n",
    "    if embedder is None or not imgs:\n",
    "        return None\n",
    "    # Warmup\n",
    "    for _ in range(n_warmup):\n",
    "        _ = embedder.encode_image(imgs[0])\n",
    "    t0 = time.time()\n",
    "    for im in imgs:\n",
    "        _ = embedder.encode_image(im)\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    ips = len(imgs) / max(dt, 1e-6)\n",
    "    mem = None\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            mem = torch.cuda.max_memory_allocated() / 1024**3\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return {\"images\": len(imgs), \"sec\": dt, \"img_per_sec\": ips, \"gpu_peak_gb\": mem}\n",
    "\n",
    "clip_prof = profile_embedder(clip, images[:16]) if clip else None\n",
    "dino_prof = profile_embedder(dino, images[:16]) if dino else None\n",
    "print(\"Profiles:\", {\"clip\": clip_prof, \"dino\": dino_prof})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) Medium-term Metrics: Mask Stability and Trajectory Drift\n",
    "\n",
    "def mask_iou(m1: np.ndarray, m2: np.ndarray) -> float:\n",
    "    inter = np.logical_and(m1>0, m2>0).sum()\n",
    "    union = np.logical_or(m1>0, m2>0).sum() + 1e-6\n",
    "    return float(inter/union)\n",
    "\n",
    "# Placeholder demo: if we created a box mask, compare the same mask to itself\n",
    "if 'm' in globals():\n",
    "    print(\"Mask IoU self:\", mask_iou(m, m))\n",
    "\n",
    "# Trajectory drift: simple cumulative distance as drift proxy\n",
    "if len(poses) >= 2:\n",
    "    dists = np.linalg.norm(np.diff(np.array([P[:3,3] for P in poses]), axis=0), axis=1)\n",
    "    drift = dists.sum()\n",
    "    print(f\"Trajectory drift (proxy): {drift:.4f}\")\n",
    "else:\n",
    "    print(\"No trajectory drift metric available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) Ablations: Backbone and Resolution Sweeps (stub)\n",
    "ABLATION_MODELS = [\n",
    "    \"facebook/dinov3-convnext-tiny-pretrain-lvd1689m\",\n",
    "    \"facebook/dinov3-convnext-base-pretrain-lvd1689m\",\n",
    "    \"facebook/dinov3-vitb16-pretrain-lvd1689m\",\n",
    "]\n",
    "ABLATION_SIZES = [224, 256]\n",
    "\n",
    "print(\"Define sweeps here; use orion.backends.dino_backend.DINOEmbedder per model/size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0277c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) Save Visual Artifacts and Logs\n",
    "import platform\n",
    "meta = {\n",
    "    \"config\": CFG,\n",
    "    \"seed\": SEED,\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"torch\": versions.get(\"torch\"),\n",
    "    \"timm\": versions.get(\"timm\"),\n",
    "    \"transformers\": versions.get(\"transformers\"),\n",
    "}\n",
    "with open(OUTPUT_DIR/\"run_meta.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved metadata to\", OUTPUT_DIR/\"run_meta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b63530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Unit Tests: Determinism, Shapes, Reproducibility (lightweight)\n",
    "# These are sanity checks; integrate into tests/ as needed.\n",
    "\n",
    "def assert_between(x, a, b):\n",
    "    assert a <= x <= b, f\"Value {x} not in [{a},{b}]\"\n",
    "\n",
    "# Cosine range check\n",
    "if dino_vecs is not None and len(dino_vecs) >= 2:\n",
    "    S = cosine_sim_matrix(dino_vecs)\n",
    "    assert np.isfinite(S).all()\n",
    "    assert_between(float(S.min()), -1.01, 1.01)\n",
    "    assert_between(float(S.max()), -1.01, 1.01)\n",
    "    print(\"Determinism/light checks passed for cosine sim\")\n",
    "else:\n",
    "    print(\"Skip checks (insufficient embeddings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) Config Switch: Full-Accuracy vs Realtime Mode\n",
    "\n",
    "def set_mode(mode: str = \"accuracy\"):\n",
    "    if mode == \"accuracy\":\n",
    "        CFG.update({\n",
    "            \"precision\": \"fp32\",\n",
    "            \"deterministic\": True,\n",
    "            \"batch_size\": 4,\n",
    "            \"img_size\": 256,\n",
    "        })\n",
    "    elif mode == \"realtime\":\n",
    "        CFG.update({\n",
    "            \"precision\": \"bf16\",\n",
    "            \"deterministic\": False,\n",
    "            \"batch_size\": 16,\n",
    "            \"img_size\": 224,\n",
    "        })\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'accuracy' or 'realtime'\")\n",
    "    print(\"Mode set:\", mode, CFG)\n",
    "\n",
    "set_mode(\"accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
