HyperGLM: HyperGraph for Video Scene Graph Generation and AnticipationTrong-Thuan Nguyen1†, Pha Nguyen1†, Jackson Cothren1, Alper Yilmaz2, Khoa Luu11University of Arkansas2Ohio State University1{thuann, panguyen, jcothre, khoaluu}@uark.edu2yilmaz.15@osu.eduuark-cviu.github.io/projects/HyperGLMFigure 1. Our HyperGLM framework supports Video Scene Graph Generation, Anticipation, and Reasoning. HyperGLM constructs scene graphs fromobserved video frames and predicts relationships in unseen frames by leveraging a unified hypergraph for temporal modeling and comprehensive understanding.AbstractMultimodal LLMs have advanced vision-language tasks butstill struggle with understanding video scenes. To bridge thisgap, Video Scene Graph Generation (VidSGG) has emergedto capture multi-object relationships across video frames.However, prior methods rely on pairwise connections, limit-ing their ability to handle complex multi-object interactionsand reasoning. To this end, we propose the MultimodalLarge Language Models (LLMs) on a Scene HyperGraph(HyperGLM), promoting reasoning about multi-way interac-tions and higher-order relationships. Our approach uniquelyintegrates entity scene graphs, which capture spatial relation-ships between objects, with a procedural graph that modelstheir causal transitions, forming a unified HyperGraph. Sig-nificantly, HyperGLM enables reasoning by injecting thisunified HyperGraph into LLMs. Additionally, we introduce anew Video Scene Graph Reasoning (VSGR) dataset featuring1.9M frames from third-person, egocentric, and drone viewsand support five tasks. Empirically, HyperGLM consistentlyoutperforms state-of-the-art methods, effectively modelingand reasoning complex relationships in diverse scenes.† Equal contribution.1. IntroductionIn recent years, Multimodal Large Language Models(LLMs) [2, 10] have set new benchmarks, with vision-language models excelling in diverse multimodal tasks. How-ever, fully understanding dynamic video scenes remains asignificant challenge for applications like autonomous driv-ing, intelligent surveillance, human-object interaction, andmultimedia analysis. Towards this goal, Video Scene GraphGeneration (VidSGG) [18, 52] has emerged as a critical taskfor capturing multi-object relationships across video frames.In particular, VidSGG enables high-level tasks such as eventforecasting [36, 43, 45], video captioning [24, 38, 40], andvideo question answering [20, 30, 32, 41] by constructingdetailed representations of entities and their interactions.However, prior VidSGG methods and datasets have lim-ited capacity for comprehensive video understanding. Tradi-tional scene graph-based methods [4, 34, 35, 42] only modelpairwise object relationships within single frames, makingit challenging to capture higher-order relationships and tem-poral dependencies in real-world scenarios. Additionally,existing benchmark datasets [18, 34, 35, 52] focus is primar-ily confined to Scene Graph Generation (SGG) and SceneThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.Except for this watermark, it is identical to the accepted version;the final published version of the proceedings is available on IEEE Xplore.29150
Graph Anticipation (SGA) tasks, lacking annotations forreasoning tasks such as Video Question Answering (VQA),Video Captioning (VC), and Relation Reasoning (RR).In this paper, we propose the Multimodal LLMs on aScene HyperGraph (HyperGLM) approach to promote rea-soning about multi-way interactions and higher-order rela-tionships through a unified HyperGraph and LLMs, as illus-trated in Fig. 1. To achieve this goal, we uniquely incorporateentity scene graphs, which capture spatial relationships be-tween objects, with a procedural graph that models theircausal transitions across video frames, forming a unifiedHyperGraph, as shown in Fig. 2. Significantly, our Hyper-GLM approach allows reasoning by injecting this unifiedHyperGraph into LLMs. In addition, we introduce a novelVideo Scene Graph Reasoning (VSGR) dataset, compris-ing 1.9 million video frames surpassing existing benchmarkdatasets [18, 34, 35, 47, 52] in scale and annotation depth.Specifically, our VSGR dataset includes videos from third-person, egocentric, and drone perspectives. It supports fivetasks: Scene Graph Generation, Scene Graph Anticipation,Video Question Answering, Video Captioning, and RelationReasoning. Notably, our VSGR dataset introduces a newRelation Reasoning task, setting it apart from existing videoscene graph datasets, as shown in the comparison in Table 1.Contributions of this Work. This work presents three con-tributions to advance Video Scene Graph Generation. First,we introduce Multimodal LLMs on a Scene HyperGraph,leveraging hyperedges and LLMs for reasoning about multi-way interactions and higher-order relationships. Second, wedevelop a new Video Scene Graph Reasoning dataset, sur-passing existing scale and annotation depth benchmarks. OurVSGR dataset primarily supports five tasks within diversevideo scenes. Finally, the proposed HyperGLM consistentlyoutperforms state-of-the-art methods across all five tasks.2. Related WorkIn this section, we review advances in scene graph generationand hypergraph applications in computer vision, then discussexisting limitations and the advantages of our approach.2.1. Scene Graph GenerationScene Graph Generation has significantly advanced withtransformer-based models [6, 13, 17, 25, 26, 51] that havebecome benchmarks due to their efficiency and state-of-the-art performance. Recent work [3, 21, 33] focuses onreducing bias and enhancing mean recall for rare predi-cates by integrating external knowledge and applying unbi-ased contextual augmentation, particularly in dynamic videocontexts. Open-vocabulary methods [15, 27] supported byvision-language models handle unseen object and relation-ship classes, improving generalization. Additionally, genera-tive models (e.g., diffusion-based methods [9, 53]) leveragescene graphs for efficient image and scene synthesis. More-over, spatial-temporal methods [4, 34–36, 42] effectivelycapture dynamic object relationships in videos. Recently,Large Language Models [23] have been utilized to enhancetriplet extraction and alignment in weakly supervised SGG.2.2. HyperGraphsHyperGraphs have been adopted in computer vision to modelcomplex multimodal data and capture higher-order relation-ships. Unlike traditional graph-based approaches, Hyper-Graphs connect multiple nodes through hyperedges, enablingmulti-way relationships. They enhance Graph Neural Net-works (GNNs) [12, 54] by allowing the modeling of moresophisticated interactions. Recent advancements, such as Hy-perGraph Convolution [1] and HyperGraph Attention [22],have further improved GNNs by capturing relationships be-yond simple pairwise connections. Therefore, HyperGraph-based models effectively handle temporal dependencies andcomplex interactions, significantly boosting performance intasks like accident anticipation [43], group activity recogni-tion [28, 55], and video question answering [44, 46, 50].2.3. DiscussionLimitations in Prior Methods. The methods introduced inSection 2.1, based on Progressive Feature Fusion [39, 51],Batch-Progressive Transformer Encoding [6, 13, 17, 25,26], Spatial-Temporal Context Integration [4, 34, 42], andMemory-Guided Temporal Consistency [7, 33, 35], have ad-vanced VidSGG. However, these methods struggle to modelhigher-order relationships and complex temporal dynam-ics. Specifically, Progressive Feature Fusion and Batch-Progressive Transformer Encoding are limited in capturinglong-term temporal dependencies, with the former lackinglong-term context due to frame-by-frame processing and thelatter only addressing short-term dependencies. Similarly,Spatial-Temporal Context Integration and Memory-GuidedTemporal Consistency inadequately represent multi-object in-teractions across video frames and insufficiently capture thetemporal evolution required for higher-order relationships.Advantages of Our Approach. Our HyperGLM approach,Multimodal LLMs on a Scene HyperGraph, promotes rea-soning about multi-way interactions and high-order rela-tionships. As illustrated in Fig. 2, HyperGLM enhancesthe model’s ability to interpret complex relationships andanticipate intricate video dynamics. Towards this goal, weuniquely integrate entity scene graphs, which is introducedin Sec. 3 to capture spatial interactions between objectsin each frame and a procedural graph, which is presentedin Sec. 4.1 to model their causal evolution. In our unified ap-proach, hyperedges connect multiple nodes to capture higher-order relationships distinguished from traditional pairwisemethods [4, 34, 35, 42]. In addition, the procedural graphenables multi-step transitions for anticipating future inter-actions or relationships, and reduces bias by generalizing29151
Figure 2. (a) To model the temporal transition, a simple approach can beusing two scene graphs Gt and Gt+1. (b) Another procedure graph canpresent this temporal modeling. (c) Our unified HyperGraph in Fig. 2cintegrates both entity scene graph to capture spatial relationships and theprocedural graph to model the temporal evolution. HyperEdge representsperson sitting on couch, holding, then playing guitar, whereas holding →playing describes a chain of interactions. HyperGraph is presented in 3D.infrequent relationship categories. Furthermore, our Hyper-GLM approach leverages fundamental mathematical proper-ties: permutation equivariance ensures that the HyperGraphstructure remains consistent under any permutation of nodelabels, and invariance to hyperedge order preserves semanticmeanings regardless of the node visit order during randomwalks which is defined in Alg. 1. The theoretical foundationsand mathematical properties are detailed in the Appendices.3. Problem FormulationIn this section, we define two tasks, including Scene GraphGeneration (SGG) and Scene Graph Anticipation (SGA).Graph vertex sequence is represented as {VG | 1 ≤t ≤T},where each set of vertex VG contains object features, bound-ing boxes, and categories. The scene graph for each frame t,denoted by Gt = VG, EG = {rij | 1 ≤i < j ≤|VG|},consists of all pairwise relationships between objects, withrij representing the relationship category between vi and vj.We aim to develop a process pθ : (VG × VG) →rij topredict the relationship rij between each object pair (vit, vjt )in VG. We define task-specific queries QSGG, and QSGA todirect our unified model to perform on each specific task. Theobjective for each task is to minimize the negative likelihoodof the predicted scene graph Gt to the truth predicate setYt = {yij | 1 ≤i < j ≤|VG|} on categories indexed by k.Scene Graph Generation (SGG) generates the scene graphfor each frame t from t = 1 to t = T, which is defined as: \label {eq:sgg_formulation } \ small \ min _\theta \mathbb {E}_{\mathbf {G}_t, Y_t} \Big [ - \sum _{(v_t^{i, j})} \sum _{k} \big (y_k^{ij} \log p_\theta (r^{ij} \mid \mathbf {Q}_{\text {SGG}})_k\big ) \Big ] (1)Scene Graph Anticipation (SGA) anticipates the scenegraphs for future frames, generating predictions Gt+n,where n denotes the anticipation horizon, formulated as: \label {eq:sga_formulation} \ sma ll \mi n  _\theta \mathbb {E}_{\keyword {\mathbf {G}_{\leq t}}, Y_t} \Big [ - \sum _{(v_{t\keyword {+n}}^{i, j})} \sum _{k} \big (y_k^{ij} \log p_\theta (r^{ij} \mid \mathbf {Q}_{\text {SGA}})_k\big ) \Big ] (2)In Eqns. (1) and (2), t indexes the current frame. While SGGpredicts the scene graph at frame t, SGA reads scene graphsup to frame t to forecast the graph in future frame t + n.Figure 3. Our HyperGLM framework comprises an image encoder, MLPprojector, temporal aggregator, unified HyperGraph, and language model.It processes video frames by encoding each frame with the image encoderand MLP, extracting spatio-temporal features through image patch grids togenerate N spatial tokens per frame. The temporal aggregator compressesthe T × N embeddings over time. The MLP projector then transformsthese visual embeddings into the language feature space as frame tokens,interleaved with language tokens, and fed into the Large Language Models.4. Our Proposed HyperGLM ApproachIn this section, we present our approach, which incorporatesa unified HyperGraph into the LLMs, as illustrated in Fig. 3.4.1. Video Scene (Hyper)GraphsTraditional VidSGG methods [4, 34, 42] construct scenegraphs Gt to represent scene entities and their relationshipsas in Fig. 2a. However, these graph-based approaches do notcapture higher-order relationships with temporal dependen-cies for video understanding [36]. To model this property,we propose a novel HyperGraph-based framework that con-structs a unified HyperGraph H = (VH, EH), representingspatial relationships within individual frames and temporaltransitions across frames, illustrated in Fig. 4. HyperGraphextends traditional graph structures by allowing hyperedgesto connect more than two nodes, making them particularlyeffective for modeling these higher-order relationships. Toleverage this property, our unified HyperGraph (see Fig. 2c)integrates entity scene graphs Gt for each frame, capturingspatial interactions as introduced in Sec. 3, along with aprocedural graph P that models their causal transitions, asdetailed in Sec. 4.1. We unify these components using arandom-walk HyperGraph construction, which is definedin Alg. 1 to capture higher-order connectivity patterns andstructural semantics, thereby approximating subgraph match-ing between the entity scene graphs and the procedural graph.This integration allows our approach to model current sub-jects’ interactions and anticipate their future relationships.Procedural Graph Construction. SGA objective is to pre-dict the set of relationships EG in the next frame based onthe current set of relationships. To model the temporal evo-lution of causal relationships between objects across videoframes, we introduce a procedural graph P = (VP, EP),serving as the temporal counterpart to the entity scene graphsGt as shown in Fig. 2b. In particular, the procedural graph Pcan vary across datasets, modeling relationship transitions.Toward this goal, we model and denote the set of verticesin P as VP = {rijt∈VG | 1 ≤t ≤T}̸=, representingdistinct relationship categories. The set of edges in P, i.e.,329152
Figure 4. Our Video Scene HyperGraph, including entity graphs and a procedural graph, as defined in Sec. 4.1. Blue nodes represent entities, whilegreen nodes denote relationships. The entity graph captures spatial relationships (subject ⊸relationship ⊸object), whereas the procedural graph modelsrelationship transitions (→). Hyperedges are visualized as polygons, encapsulating interactions through chains of relationships. For instance, a hyperedgeillustrates a person picking up, holding, opening, and reading a book while sitting on a couch. HyperGraph is presented in 3D, see Supplementary video.EP = {(rm, rn)} represent possible causal transitions be-tween these relationships, where an edge (rm, rn) indicatesthat relationship rm causally lead to relationship rn. Wequantify causal transitions by calculating transition probabil-ities w(rm, rn) via their observed frequencies as in Eqn. (3).  w(r _m,  r _n) = \frac {\sum \li m it s  _{t=1 } ^{T - 1} \sum \limits _{r ^ {ij}_{t, t+1}} \mathbbm {1}\left ( r_t^{ij} = r_m \land r_{t+1}^{ij} = r_n \right )}{ \sum \limits _{t=1}^{T - 1} \sum \limits _{r^{ij}_{t, t+1}} \mathbbm {1}\left ( r_t^{ij} = r_m \right )} \label {eq:transition_weights} (3)where 1(·) is the indicator function that counts transitionsfrom relationship rm at current frame t to rn at next framet + 1. Next, self-loops w(rm, rm) are removed from thegraph, and these probabilities are normalized as in Eqn. (4).  \sum _{r_ n \ i n E_ {\m at h bf {P}}} w(r_m, r_n) = 1 \quad \text {for all } r_m \in E_{\mathbf {P}} \label {eq:transition_norm} (4)By leveraging these probabilities, the procedural graphP enhances the prediction of future relationships. For eachrelationship rijt in frame t, the most probable relationshiprijt+1 in the next frame t + 1 is determined as in Eqn. (5).  r_{t + 1}^ {ij} = \a r g \m a x _{r_n} P(r_n \mid r_t^{ij}, v^{i, j}) \label {eq:transition_probs} (5)where P(rn | rijt , vi,j) = w(rijt , rn) × vi,j is the probabil-ity of transitioning from relationship rijt to rn, looking atobject features. This allows the model to anticipate futureinteractions based on established temporal patterns.HyperGraph Construction. To incorporate spatial rela-tionships between objects in each frame and their causaltemporal transitions, we construct a unified HyperGraph Hthat integrates the entity scene graphs Gt and the procedu-ral graph P. Specifically, a HyperGraph is an augmentedrepresentation combining the entity scene graphs with theprocedural graph. This unified structure merges spatial andtemporal relationships into a single graph, enabling the use ofconventional graph algorithms while preserving the complexinteractions captured by the HyperGraph. Mathematically, aunified HyperGraph H = (VH, EH) is defined as in Eqn. (6).  \ mathcal { H} = \left (  \bigcup _{t=1}^{T} V_{\mathbf {G}_t} \cup V_{\mathbf {P}}, \quad \bigcup _{t=1}^{T} E_{\mathbf {G}_t} \cup E_{\mathbf {P}} \right ) \label {eq:unified_HyperGraph} (6)where VH includes all entity nodes vit from each Gt and therelationship type nodes VP. The hyperedge set EH includespairwise relationships EGt within each Gt, capturing spa-tial relationships and temporal transition edges EP from P,modeling the evolution of causal relationships across frames.Random-walk Algorithm. We employ the random walksalgorithm outlined in Alg. 1 to sample representative sub-structures from the unified HyperGraph H, which capturesconnectivity patterns and mitigates the NP-hardness of exactsubgraph matching. Specifically, these walks alternate be-tween nodes and hyperedges, preserving the multi-node con-nections intrinsic to hyperedges and capturing the complex-ity of multi-object relationships and their transition acrossvideo frames. In each walk, a hyperedge hi aggregates thevisited nodes, thereby encapsulating higher-order relation-ships. For example, in the entity scene graph Gt, a walkmight traverse from a “person” to a “cup” via the “holding”,resulting in the hyperedge hi = {person, cup}. Simi-larly, within the procedural graph P, a walk might transi-tion through a sequence of interactions such as “holding”,“placing”, and “releasing”, forming the hyperedge hi ={holding, placing, releasing}. Therefore, we gen-erate sampled hyperedges Esampled = {hi | i = 1, . . . , Nw}as in L25 of Alg. 1 by conducting multiple random walks.429153
Algorithm 1 Random-walk for HyperGraph Construction.Require: H = (VH, EH), Number of Walks Nw, WalkLength NlEnsure: H′ = (VH, E′H)1: Initialize Esampled ←∅2: for i = 1 to Nw do3:Select vstart ∈VH uniformly at random4:Initialize walk sequence S ←[vstart]5:for j = 1 to Nl do6:if j is odd then\triangleright Node to HyperEdge7:vcurrent ←S[j]8:Evcurrent ←{h ∈EH | vcurrent ∈h}9:if Evcurrent ̸= ∅then10:Select hj ∈Evcurrent11:Append hj to S12:end if13:else\triangleright HyperEdge to Node14:hcurrent ←S[j]15:Vhcurrent ←hcurrent16:Select vj ∈Vhcurrent17:Append vj to S18:end if19:end for20:hi ←{v ∈S | v ∈VH}\triangleright Form new HyperEdge21:if hi /∈EH ∪Esampled then22:Esampled ←Esampled ∪{hi}23:end if24: end for25: E′H ←EH ∪Esampled26: return H′ = (VH, E′H)4.2. Multimodal LLMs on HyperGraphsFormulation. Given an input video V and a task query Q,we aim to generate a target answer sequence A of length L.Especially, by modeling a HyperGraph H, the target answerA is generated via a process illustrated in Fig. 3, defined as:  p(\m at hb f {A} | \keywor dt ri  {\mathbf {V}}, \mathbf {Q}, \keywordtwo {\mathcal {H}}) = \prod _{i=0}^{L-1} p(x_i | \keywordtri {\mathbf {V}}, \mathbf {Q}, \keywordtwo {\mathcal {H}}, x_{<i})~\label {eq:MLLM_with_G} (7)where x<i denotes the preceding token sequence, and A =[x0, . . . , xi, . . . , xL−1] is the sequence of answer tokens rea-soning video V by question Q and HyperGraph H in Fig. 4.As illustrated in Fig. 5, our process begins with the gen-eration, constructing an entity scene graph Gt = (V et , EG)for each frame t to capture detected objects and their rela-tionships. Relationship anticipation employs the proceduralgraph P to model temporal evolution and predict future in-teractions. The model then engages in reasoning using theHyperGraph H and verification through video captioning toensure contextual relevance and accuracy. It refines under-standing through clarification, generating answers by rea-soning over the video and H. In scene forecasting, it predictsTable 1. Comparisons of video scene graph datasets. SGG, SGA, VQA,VC, and RR represent Scene Graph Generation, Scene Graph Anticipation,Video Question Answering, Video Captioning, and Relation Reasoning.Dataset#FramesTasksAnnotationsSGGSGAVQAVCRRBboxRelationTextSportsHHI [47]11.4K✓✗✗✗✗✓✓✗Action Genome [18]234.3K✓✓✗✗✗✓✓✗AeroEye [35]261.5K✓✗✗✗✗✓✓✗ASPIRe [34]1.6M✓✗✗✗✗✓✓✗PVSG [52]153K✓✗✓✓✗✓✓✓VSGR (Ours)1.9M✓✓✓✓✓✓✓✓subsequent interactions by identifying new relationships andadapting to evolving contexts using transition probabilitiesin P, anticipating future relationships rijt+n and improvingconversational flow. Finally, the hypothesis allows the modelto propose and test conjectures about the underlying themesor intentions, synthesizing information from previous stepsand leveraging H to explain patterns and forecast results.5. Video Scene Graph Reasoning DatasetLimitations of Current Datasets. Existing benchmarks(see Table 1) primarily support SGG and SGA, limiting theirapplicability to reasoning tasks. They suffer from inadequatesupport for VQA, VC, and RR, shallow annotations that failto capture intricate object interactions, insufficient model-ing of temporal dynamics, and poor multimodal integration.Thus, our VSGR dataset supports SGG, SGA, VQA, VC,and RR, enabling the reasoning capabilities of LLMs.5.1. Dataset ConstructionData Acquisition Stage. We source videos from the AS-PIRe [34] and AeroEye [35] datasets. While the ASPIRedataset offers diverse, richly annotated videos emphasizingdynamic interactions and temporal changes, the AeroEyedataset offers drone-captured footage across various scenes.Comprehension Tasks via Question-Answering. We in-troduce tasks that leverage fine-grained relationships fromscene graphs, extending Scene Graph Generation to focuson relation understanding and subject/object interpretationusing <subject, relation, object> triplets andleverage GPT-4/GPT-3.5 model for language generation.Video Captioning (VC). We generate 82,532 video-caption pairs, resulting in about 22 captions per video. Ourprocess involves (1) extracting triplets from cropped videoframes focusing on foreground objects, (2) generating back-ground descriptions based on these triplets, and (3) com-bining the foreground triplets and background descriptionsto produce captions. The average length is 893 characters,surpassing the PVSG [52] dataset in quantity and detail.Video Question Answering (VQA). We develop 74,856question-answer pairs by designing questions that explorediverse relationships, selecting subject and object categoriesto verify specific relationships, or choosing triplets to assesstheir uniqueness. This results in an average of approximately20 questions per video, which exceeds existing benchmarks529154
Figure 5. An example of the diversified context within the streaming dialog in our VSGR dataset. Best viewed in color and zooming in.such as the MSRVTT-QA [49] and MSVD-QA [48] datasets.Relation Reasoning (RR). Using the annotated scenegraphs, we produce 61,120 relation reasoning tasks by se-lecting partial information as an incomplete input. Each taskrequires the model to deduce relationships among entitiesand identify the categories of the subject and object. Withan average of approximately 16 tasks per video, our VSGRdataset provides a substantial collection for evaluating mod-els’ abilities in relational reasoning and scene understanding.Question and Answer Validation. To ensure the quality andcomplexity of questions, we implement a rigorous validationprocess that combines generation by LLMs with human re-finement. Initially, GPT-4/GPT-3.5 generates queries basedon the <subject, relation, object> triplets ex-tracted from the videos. Human annotators are trained withspecific guidelines that emphasize clarity, relevance, and ap-propriate challenge levels, then review and refine questions.They enhance the questions by ensuring they are directlyanswerable from the video content and require careful rea-soning about the depicted interactions and relationships.We apply strict filtering criteria to improve the qualityof the dataset. First, we eliminate questions that do not re-quire video context and can be answered using general worldknowledge, ensuring that models must rely on visual infor-mation from the videos. Second, we exclude questions thatLLMs can answer correctly, increasing the challenge andutility of the dataset in evaluating advanced reasoning abili-ties. Finally, independent annotators perform a second roundto review and evaluate the quality of the refined questions.5.2. Dataset ComparisonAs reported in Table 1, our VSGR dataset represents a sub-stantial advancement in video scene graph benchmarks. Ourdataset comprises 3,748 videos and 1,841,243 frames, sur-passing existing datasets in scale. Unlike other datasets thataddress only a limited subset of tasks, our dataset offerscomprehensive task coverage, facilitating multifaceted eval-uations of LLMs. In addition, our ground truth enriches rela-tion annotations with comprehensive textual descriptions, en-abling sophisticated reasoning and relationship predictions,as illustrated in Fig. 5. Additionally, our VSGR datasetincorporates diverse viewpoints, including third-person, ego-centric, and drone perspectives, enhancing its generalization.6. Experiment Results6.1. Implementation DetailsDatasets. We leverage our VSGR dataset across five tasks.In addition, we utilize the PVSG [52] dataset for the SGGtask and the Action Genome [18] dataset for the SGA task.Model Configuration. We operate the CLIP-ViT-L-336 [8,37] to encode each video frame into ten tokens (one CLS to-ken and nine from 3×3 average pooling). These tokensare fed into a two-layer MLP connector to the Mistral-7B-Instruct [19] language model. For training, we applyLoRA [16] to all linear layers with a rank of 128 and a scal-ing factor of 256, omitting vision-language alignment [31].We train for two epochs with a batch size of 128 over 1629155
Figure 6. Comparison of Recall (R) and mean Recall (mR) at differentnumbers of hyperedges for the SGG task on the VSGR dataset.Table 2. Comparison (%) on the VSGR and Action Genome datasets for theScene Graph Anticipation (SGA) task at varying video input fractions F.FMethodAction GenomeVSGRR/mR@10R/mR@20R/mR@50R/mR@10R/mR@20R/mR@500.3STTran+ [5]13.9 / 3.521.6 / 7.340.8 / 20.312.0 / 4.019.0 / 8.035.7 / 18.0DSGDetr+ [11]14.3 / 3.621.8 / 7.641.3 / 21.212.5 / 4.219.5 / 8.336.0 / 19.0STTran++ [5]15.4 / 6.227.2 / 14.148.6 / 32.214.0 / 6.522.0 / 11.039.0 / 25.1DSGDetr++ [11]16.8 / 8.429.0 / 16.748.9 / 32.314.5 / 7.022.5 / 12.040.0 / 26.0SceneSayerODE [36]23.3 / 13.332.5 / 20.145.1 / 33.018.0 / 9.526.0 / 15.042.0 / 30.0SceneSayerSDE [36]25.9 / 15.635.7 / 23.147.4 / 37.119.5 / 11.027.5 / 17.044.0 / 33.5HyperGraph (Ours)26.5 / 14.836.2 / 22.149.3 / 37.218.8 / 9.327.2 / 16.342.5 / 27.4HyperGLM (Ours)27.5 / 15.837.0 / 24.550.0 / 38.019.0 / 10.028.0 / 16.543.0 / 27.50.5STTran+ [5]14.9 / 3.722.6 / 7.642.9 / 21.413.5 / 4.221.0 / 8.537.0 / 19.0DSGDetr+ [11]15.2 / 3.923.1 / 8.043.3 / 22.213.8 / 4.521.5 / 9.038.0 / 20.0STTran++ [5]16.6 / 6.629.1 / 14.751.5 / 33.415.5 / 7.023.5 / 12.541.0 / 26.5DSGDetr++ [11]17.4 / 8.430.5 / 17.051.9 / 33.916.0 / 7.524.0 / 13.042.0 / 28.0SceneSayerODE [36]26.4 / 14.336.6 / 21.449.8 / 36.020.5 / 11.029.5 / 16.546.1 / 32.5SceneSayerSDE [36]28.4 / 16.338.6 / 25.151.4 / 39.921.5 / 12.531.0 / 18.548.0 / 35.7HyperGraph (Ours)29.2 / 16.439.3 / 23.252.1 / 38.720.3 / 12.229.8 / 17.146.2 / 33.1HyperGLM (Ours)30.0 / 17.040.5 / 27.553.5 / 40.521.5 / 11.531.5 / 18.546.5 / 30.00.7STTran+ [5]16.6 / 4.225.1 / 8.547.2 / 24.015.0 / 5.024.0 / 10.541.0 / 21.5DSGDetr+ [11]16.8 / 4.325.3 / 8.847.4 / 24.715.5 / 5.224.5 / 11.042.0 / 22.0STTran++ [5]19.0 / 7.732.8 / 17.156.8 / 36.817.0 / 8.027.0 / 14.045.0 / 29.0DSGDetr++ [11]19.8 / 9.534.1 / 19.256.7 / 37.217.5 / 8.528.0 / 15.046.1 / 30.0SceneSayerODE [36]32.1 / 16.542.8 / 24.455.6 / 39.623.5 / 13.033.5 / 19.051.0 / 36.0SceneSayerSDE [36]33.3 / 18.144.0 / 27.356.4 / 44.424.5 / 14.535.7 / 21.053.0 / 38.0HyperGraph (Ours)34.3 / 19.245.2 / 25.357.2 / 42.122.3 / 13.234.2 / 19.350.4 / 35.3HyperGLM (Ours)35.7 / 19.546.1 / 30.058.2 / 44.025.1 / 13.535.5 / 21.551.0 / 33.5iterations on 4 × GPUs, taking approximately six hours.Metrics. We evaluate the SGG and SGA tasks using theRecall and mean Recall scores. In addition, we evaluatethe VQA and RR tasks by Accuracy, Precision, Recall, andF1 scores. For the VC task, we utilize CIDEr, MENTOR,ROUGE-L, and BLEU-4 scores to validate our performance.Settings. For the SGG and SGA tasks, we adopt the eval-uation settings based on [18, 36]. In particular, the modelis provided with raw video frames and must detect objectsusing a pre-trained detector (i.e. Faster R-CNN) and predictor anticipate their relationships. Especially for the SGA, weset the initial video input fraction (F) to 0.9, following [36].6.2. Ablation StudyHypergraph Parameters. The number of walks (Nw) andwalk length (Nl) are introduced in Alg. 1, directly impactingthe capacity to capture high-order relationships by deter-mining the number of hyperedges. Although more hyper-edges increase relational diversity, they can also introduceredundancy beyond an optimal point. Specifically, a higherNw broadens the range of sampled relationships, while amoderate Nl balances depth. Our experiments demonstratethat optimal performances are achieved with 60 hyperedges(Nw = 60 and Nl = 7), shown in Fig. 6. Further experi-ments on the SGG task are provided in Table 3, and experi-ments on these parameters are included in the Appendices.Video Input Fraction. We adjust the initial video inputTable 3. Comparison (%) on the VSGR and PVSG datasets for the SceneGraph Generation (SGG) task at Recall (R) and mean Recall (mR).MethodPVSGVSGRR/mR@20R/mR@50R/mR@100R/mR@20R/mR@50R/mR@100Transformer [52]4.0 / 1.84.4 / 1.94.9 / 2.025.7 / 6.334.5 / 6.543.5 / 7.0HIG [34]4.6 / 1.94.9 / 2.15.4 / 2.223.8 / 5.731.1 / 5.940.4 / 6.9CYCLO [35]5.8 / 2.06.1 / 2.26.7 / 2.329.4 / 7.136.4 / 7.747.7 / 7.7HyperGraph (Ours)6.5 / 2.27.0 / 2.47.5 / 2.631.6 / 7.838.8 / 8.350.3 / 8.5HyperGLM (Ours)7.5 / 2.88.1 / 3.78.5 / 3.935.8 / 9.242.3 / 10.154.7 / 10.4Table 4. Comparison (%) on the VSGR and Action Genome datasets for theScene Graph Anticipation (SGA) task at Recall (R) and mean Recall (mR).MethodAction GenomeVSGRR/mR@10R/mR@20R/mR@50R/mR@10R/mR@20R/mR@50STTran+ [5]17.5 / 4.626.8 / 9.249.6 / 24.316.5 / 5.526.0 / 11.043.0 / 23.0DSGDetr+ [11]17.9 / 4.727.7 / 9.751.4 / 25.917.0 / 6.027.0 / 11.544.5 / 24.5STTran++ [5]20.2 / 8.935.7 / 18.460.2 / 38.819.0 / 9.530.0 / 15.549.0 / 31.0DSGDetr++ [11]22.2 / 11.437.1 / 21.061.0 / 39.519.5 / 10.031.0 / 16.050.0 / 32.5SceneSayerODE [36]36.6 / 17.848.3 / 27.461.3 / 43.426.5 / 14.037.5 / 20.055.5 / 38.0SceneSayerSDE [36]37.3 / 20.848.6 / 30.961.6 / 46.827.5 / 16.038.5 / 22.058.2 / 40.0HyperGraph (Ours)37.5 / 19.149.3 / 31.462.3 / 47.428.4 / 17.539.3 / 22.457.6 / 41.5HyperGLM (Ours)38.8 / 22.351.5 / 33.065.2 / 48.630.2 / 18.141.1 / 23.559.3 / 43.4fraction, F, to 0.3, 0.5, and 0.7 for the SGA task. This ad-justment allows the model to learn from varying observedportions and predict the unseen segment. Table 2 indicatesthat increasing the portion of the seen video improves perfor-mance, suggesting that additional visual context is beneficial.In addition, Table 4 further confirms that the default settingat a higher input fraction (F = 0.9) leads to optimal per-formance. We also present additional settings with varyingvideo input fractions for the SGA task in the Appendices.6.3. Comparison with State-of-the-ArtsTable 3 demonstrates that HyperGLM significantly outper-forms existing methods on the PVSG and VSGR datasets,achieving the highest R@20 scores of 7.5% and 35.8%,respectively. By leveraging hyperedges connecting multi-ple nodes within the HyperGraph, HyperGLM effectivelycaptures complex object interactions and spatial dependen-cies, transcending traditional pairwise methods to gener-ate more accurate and detailed scene graph representations.Notably, integrating a procedural graph within the Hyper-Graph reduces bias. Our approach enhances mean Recall,reaching improvements of 2.8% on the PVSG dataset and9.2% on the VSGR dataset, thereby addressing the long-tail distribution challenges that have struggled in previousmethods [34, 35, 52]. Furthermore, the LLM enhances thecapacity of HyperGLM to infer and predict intricate relation-ships embedded within the unified HyperGraph, resultingin improved performance compared to HyperGraph, whichshows a decrease of 4.2% at R@20 on the VSGR dataset.As shown in Table 4, our HyperGLM approach outper-forms existing SGA methods on the Action Genome andVSGR datasets, achieving R@10 scores of 35.7% and 25.1%,respectively. This improvement stems from integrating pro-cedural graphs that model causal relationships within theHyperGraph structure. In contrast, the SceneSayer [36]method relies on NeuralODE and NeuralSDE to capture thelatent dynamics of object interaction evolution. Our pro-cedural graphs enable multi-step transitions that explicitly29156
Figure 7. Qualitative comparison of our HyperGLM approach versus SceneSayerSDE [36] for SGG and SGA. Red and green edge labels denote incorrect andcorrect predictions, respectively. Our HyperGLM approach effectively captures the evolving interactions between person-1 and bicycle-1 or person-2 andperson-3 over time and anticipates interactions in unseen video frames, while SceneSayerSDE confuses to similar predicates. Best viewed in color.Table 5. Comparison (%) on VSGR for the Video Question Answering.MethodAccuracyPrecisionRecallF1 ScoreVideo-ChatGPT [32]33.235.132.333.6Video-LLaVA-7B [30]43.143.841.742.7MovieChat [41]43.544.242.643.4Chat-UniVi-7B [20]44.345.643.244.4HyperGLM (Ours)45.447.244.345.7Table 6. Comparison (%) on VSGR for the Video Captioning.MethodCIDErMENTORROUGE-LBLEU-4MV-GPT [38]57.137.562.547.2CoCap [40]54.329.461.842.8UniVL + MELTR [24]50.528.160.042.1HyperGLM (Ours)54.530.764.948.8capture the temporal evolution of relationships by modelingsequential interactions and their dependencies. In contrast,NeuralODE and NeuralSDE primarily focus on continuous-time dynamics, which may limit their effectiveness in han-dling discrete, multi-step relational changes as illustratedin Fig. 7b. Significantly, the HyperGraph is injected intothe LLM, allowing the model to capture complex temporalpatterns, resulting in better results than the HyperGraph.In addition to the improvements in SGG and SGA shownin Fig. 7, Tables 5, 6, and 7 demonstrate the significant im-provements using our HyperGLM approach in the VQA,VC, and RR tasks. In VQA, the hyperedges connect mul-tiple objects, enabling HyperGLM to capture context-richinteractions more effectively, reaching an accuracy of 45.4%.For VC, the HyperGraph structure supports evolving objectconnections over time, allowing our HyperGLM approachto generate captions that describe the relationship betweenobjects and capture the temporal flow of interactions coher-Table 7. Comparison (%) on VSGR for the Relation Reasoning.MethodAccuracyPrecisionRecallF1 ScoreVideo-LLaVA-7B [30]41.342.540.241.3MA-LMM [14]42.843.741.842.7LLaMA-VID-7B [29]44.145.243.544.3HyperGLM (Ours)47.248.446.547.4ently, achieving scores of 54.5% at CIDEr and 64.9% atROUGE-L. In RR task, the HyperGraph effectively managesintricate dependencies between multiple objects, resulting inprecise relational inferences with an accuracy of 47.2%.7. ConclusionIn this paper, we have introduced HyperGLM, a novelVidSGG method that integrates scene hypergraph informa-tion into LLMs for context-aware and precise scene inter-pretation. Our approach effectively models complex interac-tions and higher-order relationships. It outperforms leadingmethods on benchmarks, including PVSG, Action Genome,and our newly collected VSGR dataset across five tasks.Limitations. Although our HyperGLM approach effectivelymodels multi-way interactions, managing many objects andtheir interactions can complicate relationship structures. Asthe HyperGraph expands, essential relationships may be-come obscured, reducing the clarity of scene interpretation.Acknowledgment. This material is based upon work sup-ported by the National Science Foundation under AwardNo. OIA-1946391. We also acknowledge the ArkansasHigh-Performance Computing Center for providing GPUs.29157
References[1] Song Bai, Feihu Zhang, and Philip HS Torr. Hypergraphconvolution and hypergraph attention. Pattern Recognition,110:107637, 2021. 2[2] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin,Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, DongxingMao, and Mike Zheng Shou. Videollm-online: Online videolarge language model for streaming video. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 18407–18418, 2024. 1[3] Zhanwen Chen, Saed Rezayi, and Sheng Li. More knowledge,less bias: Unbiasing scene graph generation with explicitontological adjustment. In Proceedings of the IEEE/CVFWinter Conference on Applications of Computer Vision, pages4023–4032, 2023. 2[4] Yuren Cong, Wentong Liao, Hanno Ackermann, Bodo Rosen-hahn, and Michael Ying Yang. Spatial-temporal transformerfor dynamic scene graph generation.In Proceedings ofthe IEEE/CVF international conference on computer vision,pages 16372–16382, 2021. 1, 2, 3[5] Yuren Cong, Wentong Liao, H. Ackermann, M. Yang, andB. Rosenhahn. Spatial-temporal transformer for dynamicscene graph generation. IEEE International Conference onComputer Vision, 2021. 7[6] Yuren Cong, Michael Ying Yang, and Bodo Rosenhahn.Reltr: Relation transformer for scene graph generation. IEEETransactions on Pattern Analysis and Machine Intelligence,45(9):11169–11183, 2023. 2[7] Youming Deng, Yansheng Li, Yongjun Zhang, Xiang Xi-ang, Jian Wang, Jingdong Chen, and Jiayi Ma. Hierarchicalmemory learning for fine-grained scene graph generation. InEuropean Conference on Computer Vision, pages 266–283.Springer, 2022. 2[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image isworth 16x16 words: Transformers for image recognition atscale. In International Conference on Learning Representa-tions, 2021. 6[9] Azade Farshad, Yousef Yeganeh, Yu Chi, Chengzhi Shen,Böjrn Ommer, and Nassir Navab. Scenegenie: Scene graphguided diffusion models for image synthesis. In Proceed-ings of the IEEE/CVF International Conference on ComputerVision, pages 88–98, 2023. 2[10] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, MeishanZhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought:Step-by-step video reasoning from perception to cognition.In Forty-first International Conference on Machine Learning,2024. 1[11] Shengyu Feng, Hesham Mostafa, Marcel Nassar, SomdebMajumdar, and Subarna Tripathi. Exploiting long-term de-pendencies for generating dynamic scene graphs. IEEE Work-shop/Winter Conference on Applications of Computer Vision,2021. 7[12] Yue Gao, Yifan Feng, Shuyi Ji, and Rongrong Ji. Hgnn+:General hypergraph neural networks. IEEE Transactions onPattern Analysis and Machine Intelligence, 45(3):3181–3199,2022. 2[13] Zeeshan Hayder and Xuming He.Dsgg: Dense relationtransformer for an end-to-end scene graph generation. In Pro-ceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 28317–28326, 2024. 2[14] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xue-fei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-NamLim. Ma-lmm: Memory-augmented large multimodal modelfor long-term video understanding. In Proceedings of theIEEE/CVF Conference on Computer Vision and PatternRecognition, pages 13504–13514, 2024. 8[15] Tao He, Lianli Gao, Jingkuan Song, and Yuan-Fang Li. To-wards open-vocabulary scene graph generation with prompt-based finetuning.In European Conference on ComputerVision, pages 56–73. Springer, 2022. 2[16] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.LoRA: Low-rank adaptation of large language models. InInternational Conference on Learning Representations, 2022.6[17] Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee,and Seunghyun Park. Egtr: Extracting graph from transformerfor scene graph generation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 24229–24238, 2024. 2[18] Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan CarlosNiebles. Action genome: Actions as compositions of spatio-temporal scene graphs. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 10236–10247, 2020. 1, 2, 5, 6, 7[19] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,Florian Bressand, Gianna Lengyel, Guillaume Lample, LucileSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825,2023. 6[20] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao,and Li Yuan. Chat-univi: Unified visual representation em-powers large language models with image and video un-derstanding. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 13700–13710, 2024. 1, 8[21] Tianlei Jin, Fangtai Guo, Qiwei Meng, Shiqiang Zhu, Xi-angming Xi, Wen Wang, Zonghao Mu, and Wei Song. Fastcontextual scene graph generation with unbiased context aug-mentation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 6302–6311,2023. 2[22] Eun-Sol Kim, Woo Young Kang, Kyoung-Woon On, Yu-JungHeo, and Byoung-Tak Zhang. Hypergraph attention networksfor multimodal learning. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages14581–14590, 2020. 2[23] Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, YeonjunIn, Jinyoung Moon, Donghyun Kim, and Chanyoung Park.Llm4sgg: Large language models for weakly supervisedscene graph generation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 28306–28316, 2024. 2[24] Dohwan Ko, Joonmyung Choi, Hyeong Kyu Choi, Kyoung-29158
Woon On, Byungseok Roh, and Hyunwoo J Kim. Meltr: Metaloss transformer for learning to fine-tune video foundationmodels. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 20105–20115,2023. 1, 8[25] Sanjoy Kundu and Sathyanarayanan N Aakur. Is-ggt: Iter-ative scene graph generation with generative transformers.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 6292–6301, 2023. 2[26] Rongjie Li, Songyang Zhang, and Xuming He. Sgtr: End-to-end scene graph generation with transformer. In proceedingsof the IEEE/CVF conference on computer vision and patternrecognition, pages 19486–19496, 2022. 2[27] Rongjie Li, Songyang Zhang, Dahua Lin, Kai Chen, andXuming He. From pixels to graphs: Open-vocabulary scenegraph generation with vision-language models. In Proceed-ings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 28076–28086, 2024. 2[28] Wanxin Li, Wei Xie, Zhigang Tu, Wei Wang, and LianghaoJin. Multi-hyperedge hypergraph for group activity recog-nition. In 2022 International Joint Conference on NeuralNetworks (IJCNN), pages 01–07. IEEE, 2022. 2[29] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An im-age is worth 2 tokens in large language models. In EuropeanConference on Computer Vision, pages 323–340. Springer,2025. 8[30] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, PengJin, and Li Yuan. Video-llava: Learning united visual rep-resentation by alignment before projection. arXiv preprintarXiv:2311.10122, 2023. 1, 8[31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.Visual instruction tuning. Advances in neural informationprocessing systems, 36, 2024. 6[32] Muhammad Maaz, Hanoona Rasheed, Salman Khan, andFahad Khan. Video-ChatGPT: Towards detailed video un-derstanding via large vision and language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Pro-ceedings of the 62nd Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages12585–12602, Bangkok, Thailand, Aug. 2024. Associationfor Computational Linguistics. 1, 8[33] Sayak Nag, Kyle Min, Subarna Tripathi, and Amit K Roy-Chowdhury. Unbiased scene graph generation in videos. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 22803–22813, 2023. 2[34] Trong-Thuan Nguyen, Pha Nguyen, and Khoa Luu. Hig: Hier-archical interlacement graph approach to scene graph genera-tion in video understanding. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,2024. 1, 2, 3, 5, 7[35] Trong-Thuan Nguyen, Pha Nguyen, Li Xin, Cothren Jack-son, Yilmaz Alper, and Khoa Luu. CYCLO: Cyclic graphtransformer approach to multi-object relationship modelingin aerial videos. In The Thirty-eighth Annual Conference onNeural Information Processing Systems, 2024. 1, 2, 5, 7[36] Rohith Peddi, Saksham Singh, Parag Singla, Vibhav Gogate,et al. Towards scene graph anticipation. In European Confer-ence on Computer Vision. Springer, 2024. 1, 2, 3, 7, 8[37] Alec Radford, Jong Wook Kim, Chris Hallacy, AdityaRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-sion. In International conference on machine learning, pages8748–8763. PMLR, 2021. 6[38] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, andCordelia Schmid. End-to-end generative pretraining for mul-timodal video captioning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition,pages 17959–17968, 2022. 1, 8[39] Xindi Shang, Yicong Li, Junbin Xiao, Wei Ji, and Tat-SengChua. Video visual relation detection via iterative inference.In Proceedings of the 29th ACM international conference onMultimedia, pages 3654–3663, 2021. 2[40] Yaojie Shen, Xin Gu, Kai Xu, Heng Fan, Longyin Wen, andLibo Zhang. Accurate and fast compressed video captioning.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 15558–15567, 2023. 1, 8[41] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang,Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, TianYe, Yanting Zhang, et al. Moviechat: From dense token tosparse memory for long video understanding. In Proceedingsof the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 18221–18232, 2024. 1, 8[42] Yao Teng, Limin Wang, Zhifeng Li, and Gangshan Wu. Targetadaptive context aggregation for video scene graph generation.In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 13688–13697, 2021. 1, 2, 3[43] Nupur Thakur, PrasanthSai Gouripeddi, and Baoxin Li. Graph(graph): A nested graph-based framework for early accidentanticipation. In Proceedings of the IEEE/CVF Winter Confer-ence on Applications of Computer Vision, pages 7533–7541,2024. 1, 2[44] Aisha Urooj, Hilde Kuehne, Bo Wu, Kim Chheu, Walid Bous-selham, Chuang Gan, Niels Lobo, and Mubarak Shah. Learn-ing situation hyper-graphs for video question answering. InProceedings of the IEEE/CVF Conference on Computer Vi-sion and Pattern Recognition, pages 14879–14889, 2023. 2[45] Jiahao Wang, Guo Chen, Yifei Huang, Limin Wang, and TongLu. Memory-and-anticipation transformer for online actionunderstanding. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 13824–13835, 2023.1[46] Yanan Wang, Shuichiro Haruta, Donghuo Zeng, Julio Viz-carra, and Mori Kurokawa. Multi-object event graph repre-sentation learning for video question answering. In Meetingon Image Recognition and Understanding, 2024. 2[47] Tao Wu, Runyu He, Gangshan Wu, and Limin Wang.Sportshhi: A dataset for human-human interaction detectionin sports videos. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, 2024. 2, 5[48] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,Xiangnan He, and Yueting Zhuang. Video question answeringvia gradually refined attention over appearance and motion.In Proceedings of the 25th ACM international conference onMultimedia, pages 1645–1653, 2017. 6[49] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A largevideo description dataset for bridging video and language. InProceedings of the IEEE conference on computer vision and29159
pattern recognition, pages 5288–5296, 2016. 6[50] Zenan Xu, Wanjun Zhong, Qinliang Su, Zijing Ou, andFuwei Zhang. Modeling semantic composition with syntac-tic hypergraph for video question answering. arXiv preprintarXiv:2205.06530, 2022. 2[51] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,Wayne Zhang, and Ziwei Liu. Panoptic scene graph gen-eration. In European Conference on Computer Vision, pages178–196. Springer, 2022. 2[52] Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo,Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, WayneZhang, Chen Change Loy, et al. Panoptic video scene graphgeneration. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 18675–18685, 2023. 1, 2, 5, 6, 7[53] Guangyao Zhai, Evin Pınar Örnek, Shun-Cheng Wu, Yan Di,Federico Tombari, Nassir Navab, and Benjamin Busam. Com-monscenes: Generating commonsense 3d indoor scenes withscene graphs. Advances in Neural Information ProcessingSystems, 36, 2024. 2[54] Zizhao Zhang, Yifan Feng, Shihui Ying, and Yue Gao.Deep hypergraph structure learning.arXiv preprintarXiv:2208.12547, 2022. 2[55] Xiaolin Zhu, Dongli Wang, Jianxun Li, Rui Su, Qin Wan, andYan Zhou. Dynamical attention hypergraph convolutionalnetwork for group activity recognition. IEEE Transactionson Neural Networks and Learning Systems, 2024. 229160