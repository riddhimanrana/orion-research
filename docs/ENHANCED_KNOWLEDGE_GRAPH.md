# Enhanced Knowledge Graph System

## Overview

The Enhanced Knowledge Graph system transforms video tracking results into a rich, contextual knowledge graph with:

- **Scene/Room Detection** - Automatically classifies scenes (office, kitchen, bedroom, etc.)
- **Spatial Relationships** - Tracks which objects are near, above, below, left/right of each other
- **Contextual Embeddings** - Combines visual and spatial context for better retrieval
- **Causal Reasoning** - Infers potential cause-effect relationships between state changes
- **Multi-modal QA** - Intelligent question answering with scene understanding

## Architecture

```
Tracking Results
       â†“
Enhanced KG Builder
       â”œâ”€â”€ Scene Classifier (office, kitchen, bedroom, etc.)
       â”œâ”€â”€ Spatial Analyzer (near, above, below, contains, etc.)
       â”œâ”€â”€ Contextual Embedder (vision + spatial + scene)
       â””â”€â”€ Causal Reasoning Engine (temporal + spatial + semantic)
       â†“
Neo4j Knowledge Graph
       â”œâ”€â”€ Entity Nodes (with contextual embeddings)
       â”œâ”€â”€ Scene Nodes (with type classification)
       â”œâ”€â”€ Spatial Relationships (NEAR, ABOVE, CONTAINS, etc.)
       â”œâ”€â”€ Causal Chains (POTENTIALLY_CAUSED)
       â””â”€â”€ Scene Transitions (TRANSITIONS_TO)
       â†“
Enhanced Video QA
       â”œâ”€â”€ Question Classification (spatial, scene, temporal, causal, entity)
       â”œâ”€â”€ Context Retrieval (targeted by question type)
       â””â”€â”€ LLM Answer Generation (with rich context)
```

## Key Features

### 1. Scene Classification

Automatically classifies scenes based on object composition:

**Scene Types:**
- `office` - Desk, computer, keyboard, mouse, monitor
- `kitchen` - Oven, refrigerator, sink, microwave
- `living_room` - Couch, TV, chair
- `bedroom` - Bed, clock, lamp
- `bathroom` - Toilet, sink
- `dining_room` - Dining table, chairs
- `outdoor` - Trees, cars, traffic lights
- `workspace` - Laptop, keyboard, minimal setup

**Algorithm:**
```python
# Each scene type has required and common objects
SCENE_PATTERNS = {
    'office': {
        'required': {'laptop', 'keyboard', 'mouse', 'monitor'},
        'common': {'chair', 'book', 'cup', 'phone'},
        'weight': 1.0
    },
    ...
}

# Scoring: 70% required match + 30% common match
score = (required_ratio * 0.7 + common_ratio * 0.3) * weight
```

### 2. Spatial Relationships

Analyzes spatial relationships between entities:

**Relationship Types:**
- `very_near` - Distance < 15% of frame diagonal
- `near` - Distance < 30% of frame diagonal  
- `same_region` - Distance < 50% of frame diagonal
- `above` / `below` - Vertical positioning
- `left_of` / `right_of` - Horizontal positioning
- `contains` / `inside` - Bounding box containment

**Properties:**
- `confidence` - Relationship confidence (0-1)
- `co_occurrence_count` - How many times seen together
- `avg_distance` - Average normalized distance

### 3. Contextual Embeddings

Generates rich embeddings combining multiple signals:

```python
contextual_embedding = (
    0.6 * visual_embedding +      # CLIP visual features
    0.4 * textual_embedding       # Scene + spatial context
)

# Textual context includes:
# - Object description
# - Scene type ("Located in office")
# - Surrounding objects ("Surrounded by keyboard, mouse, monitor")
# - Spatial zone ("Typically found in center")
# - Relationships ("Often near keyboard")
```

### 4. Causal Reasoning

Infers potential causal relationships using:

**Temporal Constraint:**
- Cause must happen before effect
- Within reasonable time window (< 5 seconds)

**Spatial Constraint:**
- Entities must be reasonably close
- Uses co-occurrence and proximity scores

**Semantic Plausibility:**
- Agents (person, dog, car) more likely to cause changes
- Static objects less likely to cause changes

**Confidence Scoring:**
```python
confidence = (
    0.3 * temporal_score +     # Closer in time = higher
    0.4 * spatial_score +      # Closer in space = higher  
    0.3 * semantic_score       # Agent acting on object = higher
)
```

### 5. Enhanced QA System

Intelligently answers questions with context-aware retrieval:

**Question Types:**
- `spatial` - "Where is X?", "What's near Y?"
- `scene` - "What room is this?", "What's the setting?"
- `temporal` - "When did X happen?", "What happened after Y?"
- `causal` - "Why did X change?", "What caused Y?"
- `entity` - "Tell me about X", "Describe the Y"
- `general` - Overview questions

**Retrieval Strategy:**
Each question type triggers specialized context retrieval from the knowledge graph.

## Graph Schema

### Nodes

**Entity Node:**
```cypher
(:Entity {
    id: string,
    class: string,
    description: string,
    appearance_count: int,
    first_seen: float,
    last_seen: float,
    spatial_zone: string,
    scene_types: [string]
})
```

**Scene Node:**
```cypher
(:Scene {
    id: string,
    scene_type: string,
    confidence: float,
    frame_start: int,
    frame_end: int,
    timestamp_start: float,
    timestamp_end: float,
    description: string,
    dominant_objects: [string]
})
```

### Relationships

**APPEARS_IN:**
```cypher
(Entity)-[:APPEARS_IN]->(Scene)
```

**SPATIAL_REL:**
```cypher
(Entity)-[:SPATIAL_REL {
    type: string,
    confidence: float,
    co_occurrence: int,
    avg_distance: float
}]->(Entity)
```

**POTENTIALLY_CAUSED:**
```cypher
(Entity)-[:POTENTIALLY_CAUSED {
    temporal_gap: float,
    spatial_proximity: float,
    confidence: float,
    cause_state: string,
    effect_state: string
}]->(Entity)
```

**TRANSITIONS_TO:**
```cypher
(Scene)-[:TRANSITIONS_TO {
    frame_gap: int
}]->(Scene)
```

## Usage

### 1. Build Enhanced Knowledge Graph

```python
from src.orion.enhanced_knowledge_graph import EnhancedKnowledgeGraphBuilder
import json

# Load tracking results
with open('tracking_results.json') as f:
    tracking_results = json.load(f)

# Build enhanced knowledge graph
builder = EnhancedKnowledgeGraphBuilder()
stats = builder.build_from_tracking_results(tracking_results)
builder.close()

print(f"Entities: {stats['entities']}")
print(f"Scenes: {stats['scenes']}")
print(f"Spatial Relationships: {stats['spatial_relationships']}")
print(f"Causal Chains: {stats['causal_chains']}")
```

### 2. Query with Enhanced QA System

```python
from src.orion.enhanced_video_qa import EnhancedVideoQASystem

# Create QA system
qa = EnhancedVideoQASystem()

# Ask questions
answer = qa.ask_question("What type of room is this?")
print(answer)

answer = qa.ask_question("What objects are near the laptop?")
print(answer)

answer = qa.ask_question("What happened during the video?")
print(answer)
```

### 3. Interactive Session

```bash
# Start interactive QA session
python -m src.orion.enhanced_video_qa

# Or use the test script
python scripts/test_enhanced_kg.py --interactive
```

### 4. Direct Cypher Queries

```cypher
// Find all scenes of a specific type
MATCH (s:Scene)
WHERE s.scene_type = 'office'
RETURN s.description, s.dominant_objects

// Find spatial relationships
MATCH (a:Entity)-[r:SPATIAL_REL]->(b:Entity)
WHERE r.type = 'near'
RETURN a.class, b.class, r.confidence
ORDER BY r.confidence DESC

// Find potential causal chains
MATCH (cause:Entity)-[r:POTENTIALLY_CAUSED]->(effect:Entity)
RETURN cause.class, effect.class, r.confidence, r.temporal_gap
ORDER BY r.confidence DESC

// Find scene transitions
MATCH (s1:Scene)-[:TRANSITIONS_TO]->(s2:Scene)
RETURN s1.scene_type, s2.scene_type, s1.timestamp_end, s2.timestamp_start

// Find entities by scene type
MATCH (e:Entity)-[:APPEARS_IN]->(s:Scene)
WHERE s.scene_type = 'bedroom'
RETURN DISTINCT e.class, count(s) as scene_count
ORDER BY scene_count DESC
```

## Test Results

From `video1.mp4` analysis:

```
âœ“ Knowledge graph built successfully!
  Entities: 21
  Scenes: 9
  Spatial Relationships: 24
  Causal Chains: 0
  Scene Transitions: 8
```

**Scene Detection:**
- 4 Bedroom scenes (confidence: 0.83)
- 2 Workspace scenes (confidence: 0.64)
- 3 Unknown scenes (mixed objects)

**Spatial Relationships:**
- 24 co-occurrence relationships detected
- Examples: tv â†” keyboard, keyboard â†” mouse, person â†” bed

**Question Answering:**
- âœ“ Scene questions: "What type of room?" â†’ "Primarily bedroom scenes"
- âœ“ Object questions: "What objects?" â†’ "People, beds, TVs, keyboards, mice"
- âœ“ Temporal questions: "What happened?" â†’ Detailed timeline with transitions
- âš  Spatial questions: Limited by lack of laptop detection in test video

## Performance

**Knowledge Graph Construction:**
- Scene detection: ~0.1s per scene
- Spatial analysis: ~0.5s for 21 entities
- Causal reasoning: ~0.2s for state change analysis
- Neo4j ingestion: ~1s for all nodes and relationships

**Query Performance:**
- Simple entity queries: ~10ms
- Spatial relationship queries: ~20ms
- Scene classification queries: ~15ms
- Complex multi-hop queries: ~50ms

**Memory Usage:**
- Minimal overhead (shares ModelManager singleton)
- Neo4j graph: ~1MB per 100 entities

## Future Enhancements

### Short Term
1. âœ… Scene classification
2. âœ… Spatial relationships
3. âœ… Enhanced QA system
4. âš  Causal reasoning (needs state change data)
5. ðŸ”„ Contextual embeddings (basic implementation)

### Medium Term
1. **Bbox-level spatial analysis** - Use actual bounding boxes for precise relationships
2. **Temporal clustering** - Group similar temporal patterns
3. **Scene segmentation** - Better scene boundary detection
4. **Visual similarity indexing** - Vector search on scene embeddings
5. **Multi-entity event composition** - Group related state changes into events

### Long Term
1. **Learned causal models** - Train models to predict causality
2. **3D spatial reasoning** - Depth estimation for better spatial understanding
3. **Activity recognition** - Classify human activities in scenes
4. **Object interaction detection** - Detect when objects interact
5. **Semantic scene graphs** - Full scene graph with semantic relationships

## Example Queries

### Find co-located objects
```cypher
MATCH (e1:Entity)-[r:SPATIAL_REL]-(e2:Entity)
WHERE r.co_occurrence > 10
RETURN e1.class, e2.class, r.type, r.co_occurrence
ORDER BY r.co_occurrence DESC
```

### Find dominant objects per scene type
```cypher
MATCH (s:Scene)
RETURN s.scene_type, s.dominant_objects, count(*) as count
ORDER BY count DESC
```

### Find entities that appear across multiple scene types
```cypher
MATCH (e:Entity)-[:APPEARS_IN]->(s:Scene)
WITH e, collect(DISTINCT s.scene_type) as scene_types
WHERE size(scene_types) > 1
RETURN e.class, scene_types, e.appearance_count
ORDER BY size(scene_types) DESC
```

### Timeline of scene transitions
```cypher
MATCH path = (s1:Scene)-[:TRANSITIONS_TO*]->(sn:Scene)
WITH nodes(path) as scenes
UNWIND scenes as s
RETURN s.timestamp_start, s.timestamp_end, s.scene_type, s.dominant_objects
ORDER BY s.timestamp_start
```

## Troubleshooting

### Neo4j Connection Issues
```bash
# Check if Neo4j is running
docker ps | grep neo4j

# Start Neo4j
docker run -d --name neo4j-orion \
  -p 7687:7687 -p 7474:7474 \
  -e NEO4J_AUTH=neo4j/orion123 \
  neo4j:latest

# View Neo4j browser
open http://localhost:7474
```

### No Causal Chains Detected
This is expected if:
- No state changes detected in video (static scene)
- Entities too far apart spatially
- Temporal gaps too large (>5s)

To enable causal detection:
- Ensure state change detection is working
- Use videos with dynamic content
- Adjust thresholds in `CausalReasoningEngine`

### Scene Classification Always "Unknown"
Check object composition:
- Need minimum 30% confidence score
- Requires typical objects for scene type
- Mixed scenes may not match patterns

To improve:
- Add more scene patterns in `SceneClassifier.SCENE_PATTERNS`
- Adjust confidence threshold
- Use longer video segments for better context

## Integration with Full Pipeline

```python
# Complete pipeline from video to QA
from src.orion.tracking_engine import run_tracking_engine
from src.orion.enhanced_knowledge_graph import EnhancedKnowledgeGraphBuilder
from src.orion.enhanced_video_qa import EnhancedVideoQASystem

# 1. Run tracking engine
entities, observations = run_tracking_engine("video.mp4")

# 2. Build tracking results dict
tracking_results = {
    'video_path': 'video.mp4',
    'entities': [entity.to_dict() for entity in entities],
    'total_observations': len(observations),
    # ... other metadata
}

# 3. Build enhanced knowledge graph
builder = EnhancedKnowledgeGraphBuilder()
stats = builder.build_from_tracking_results(tracking_results)
builder.close()

# 4. Query the video
qa = EnhancedVideoQASystem()
answer = qa.ask_question("What happened in the video?")
print(answer)
```

## Credits

Built on top of:
- **YOLO11x** - Object detection
- **CLIP** - Visual embeddings and multimodal understanding
- **FastVLM** - Rich descriptions
- **Neo4j** - Knowledge graph storage
- **Gemma 3** - Question answering via Ollama

Part of the Orion Research Framework for video understanding.
