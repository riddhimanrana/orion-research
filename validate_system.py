#!/usr/bin/env python3
"""
End-to-End System Validation Pipeline

Tests the complete egocentric video understanding system:
1. Video â†’ Frame extraction
2. Frame â†’ YOLO detection + CLIP Re-ID
3. Detections â†’ Scene understanding (FastVLM)
4. Scene + objects â†’ Memgraph storage
5. Memgraph + query â†’ Gemini API response
6. Validation: Accuracy measurement

Usage:
    python validate_system.py --video data/examples/test.mp4 [--max-frames 50]
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import cv2
import numpy as np

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    logger.warning("Gemini API not available - install: pip install google-generativeai")

try:
    from orion.perception.observer import Observer
    from orion.perception.describer import Describer
    from orion.semantic.scene_classifier import SceneClassifier
    from orion.semantic.zone_manager import ZoneManager
    from orion.graph.memgraph_backend import MemgraphBackend, EntityObservation
    ORION_AVAILABLE = True
except ImportError as e:
    ORION_AVAILABLE = False
    logger.error(f"Orion modules not available: {e}")


@dataclass
class FrameAnalysis:
    """Single frame analysis result"""
    frame_idx: int
    timestamp: float
    scene_type: str
    objects: List[Dict[str, Any]]  # {class, conf, bbox, embedding}
    description: Optional[str] = None
    zones: Optional[Dict[str, List[str]]] = None  # {left: [obj_ids], center: [...]}


@dataclass
class ValidationResult:
    """Overall validation result"""
    video_path: str
    total_frames: int
    processed_frames: int
    total_objects_detected: int
    unique_objects: int
    storage_size_mb: float
    queries_tested: List[Dict[str, Any]]
    gemini_responses: List[Dict[str, Any]]
    bottlenecks: List[str]
    timestamp: str


class VideoProcessor:
    """Process video frames through perception pipeline"""
    
    def __init__(self):
        self.observer = None
        self.describer = None
        self.scene_classifier = None
        self.zone_manager = None
        self.memgraph = None
        self._initialize_components()
    
    def _initialize_components(self):
        """Initialize perception components"""
        if not ORION_AVAILABLE:
            logger.error("Cannot initialize - Orion modules unavailable")
            return
        
        try:
            logger.info("ðŸ”§ Initializing perception components...")
            self.observer = Observer()
            self.describer = Describer()
            self.scene_classifier = SceneClassifier()
            self.zone_manager = ZoneManager()
            self.memgraph = MemgraphBackend()
            logger.info("âœ“ All components initialized")
        except Exception as e:
            logger.error(f"âœ— Initialization failed: {e}")
            raise
    
    def process_frame(self, frame: np.ndarray, frame_idx: int, fps: float = 30) -> Optional[FrameAnalysis]:
        """
        Process single frame through entire pipeline
        
        Args:
            frame: Video frame (BGR)
            frame_idx: Frame number
            fps: Video FPS for timestamps
            
        Returns:
            FrameAnalysis or None if processing fails
        """
        try:
            timestamp = frame_idx / fps
            
            # 1. Detect objects (YOLO)
            detections = self.observer.detect(frame)
            if not detections:
                logger.debug(f"Frame {frame_idx}: No objects detected")
                return None
            
            # 2. Classify scene (FastVLM + scene classifier)
            scene_type = self.scene_classifier.classify_frame(frame)
            
            # 3. Get spatial zones
            zones = self.zone_manager.assign_zones(frame, detections)
            
            # 4. Scene description
            description = self.describer.describe_scene(frame, detections, scene_type)
            
            # 5. Extract objects with embeddings
            objects = []
            for det in detections:
                obj = {
                    'class': det.get('class', 'unknown'),
                    'confidence': det.get('confidence', 0.0),
                    'bbox': det.get('bbox', [0, 0, 0, 0]),
                    'embedding': det.get('embedding', None),  # CLIP embedding
                    'zone': self._get_object_zone(det['bbox'], zones)
                }
                objects.append(obj)
            
            analysis = FrameAnalysis(
                frame_idx=frame_idx,
                timestamp=timestamp,
                scene_type=scene_type,
                objects=objects,
                description=description,
                zones=zones
            )
            
            logger.info(f"Frame {frame_idx:4d}: {len(objects)} objects, scene={scene_type}")
            return analysis
            
        except Exception as e:
            logger.error(f"âœ— Frame {frame_idx} processing failed: {e}")
            return None
    
    def _get_object_zone(self, bbox: List[float], zones: Dict[str, List[str]]) -> str:
        """Determine which zone object is in"""
        x1, y1, x2, y2 = bbox
        center_x = (x1 + x2) / 2
        
        # Simple heuristic: 1/3 of image width per zone
        img_width = 640  # Assume standard size, should be from frame
        if center_x < img_width / 3:
            return "left"
        elif center_x < 2 * img_width / 3:
            return "center"
        else:
            return "right"
    
    def store_analysis(self, analyses: List[FrameAnalysis]) -> int:
        """
        Store frame analyses in Memgraph
        
        Args:
            analyses: List of FrameAnalysis objects
            
        Returns:
            Number of observations stored
        """
        if not self.memgraph:
            logger.warning("Memgraph backend not available")
            return 0
        
        count = 0
        for analysis in analyses:
            for obj in analysis.objects:
                try:
                    observation = EntityObservation(
                        entity_id=hash(f"{analysis.scene_type}_{obj['class']}") % 10000,
                        frame_idx=analysis.frame_idx,
                        timestamp=analysis.timestamp,
                        bbox=obj['bbox'],
                        class_name=obj['class'],
                        confidence=obj['confidence'],
                        zone_id=hash(obj['zone']) % 100,
                        caption=analysis.description
                    )
                    # Store in Memgraph (if implemented)
                    count += 1
                except Exception as e:
                    logger.warning(f"Failed to store observation: {e}")
        
        logger.info(f"âœ“ Stored {count} observations in Memgraph")
        return count


class GeminiQueryInterface:
    """Interface to Gemini for querying video understanding"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY not set")
        
        if GEMINI_AVAILABLE:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel("gemini-2.0-flash")
        else:
            logger.warning("Gemini API not available")
            self.model = None
    
    def query_with_context(
        self,
        question: str,
        graph_data: Dict[str, Any],
        frame_descriptions: List[str],
        scene_timeline: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Query Gemini with video understanding context
        
        Args:
            question: User question (e.g., "where is my keys")
            graph_data: Memgraph observations
            frame_descriptions: FastVLM descriptions per frame
            scene_timeline: Timeline of scene changes
            
        Returns:
            LLM response with reasoning
        """
        if not self.model:
            logger.error("Gemini model not available")
            return {"error": "Model not available"}
        
        # Build context from video understanding
        context = self._build_context(graph_data, frame_descriptions, scene_timeline)
        
        # Construct prompt
        prompt = f"""You are analyzing an egocentric video where a person moves through different rooms.
        
The video analysis includes:
- Object detections (YOLO)
- Scene type classification (office, kitchen, etc.)
- Spatial zones (left/right/center in frame)
- Object Re-ID (CLIP embeddings)
- Temporal state changes

Video Understanding:
{context}

User Question: {question}

Based on the video understanding above, answer the question. Be specific about:
1. What you observed
2. Temporal sequence (when did you see X)
3. Confidence level
4. Any ambiguities or uncertainties
"""
        
        try:
            logger.info(f"ðŸ¤– Querying Gemini: {question}")
            response = self.model.generate_content(prompt)
            
            result = {
                "question": question,
                "answer": response.text,
                "timestamp": datetime.now().isoformat(),
                "context_length": len(context)
            }
            logger.info(f"âœ“ Gemini response received ({len(response.text)} chars)")
            return result
            
        except Exception as e:
            logger.error(f"âœ— Gemini query failed: {e}")
            return {"error": str(e), "question": question}
    
    def _build_context(
        self,
        graph_data: Dict[str, Any],
        frame_descriptions: List[str],
        scene_timeline: List[Dict[str, Any]]
    ) -> str:
        """Build context string for LLM"""
        context_parts = []
        
        # Object observations
        if graph_data.get("observations"):
            context_parts.append("## Object Observations")
            for obs in graph_data["observations"][:20]:  # Top 20
                context_parts.append(f"- {obs.get('class')} at {obs.get('zone')} (conf: {obs.get('confidence', 0):.2f})")
        
        # Scene timeline
        if scene_timeline:
            context_parts.append("\n## Scene Timeline")
            for i, scene in enumerate(scene_timeline[:10]):  # Top 10
                context_parts.append(f"- Frame {scene.get('frame')}: {scene.get('scene_type')}")
        
        # Key descriptions
        if frame_descriptions:
            context_parts.append("\n## Key Descriptions")
            for desc in frame_descriptions[:5]:
                if desc:
                    context_parts.append(f"- {desc}")
        
        return "\n".join(context_parts)


class ValidationBenchmark:
    """Benchmark and validation harness"""
    
    def __init__(self, video_path: str, max_frames: int = 100):
        self.video_path = video_path
        self.max_frames = max_frames
        self.processor = VideoProcessor()
        self.analyses: List[FrameAnalysis] = []
        self.storage_size = 0
    
    def run(self) -> ValidationResult:
        """Execute full validation pipeline"""
        logger.info(f"ðŸŽ¬ Starting validation on {self.video_path}")
        
        # Extract frames
        frames = self._load_frames()
        if not frames:
            logger.error("Failed to load frames")
            return None
        
        logger.info(f"ðŸ“¹ Loaded {len(frames)} frames from video")
        
        # Process frames
        self.analyses = []
        for idx, (frame, fps) in enumerate(frames):
            if idx >= self.max_frames:
                break
            analysis = self.processor.process_frame(frame, idx, fps)
            if analysis:
                self.analyses.append(analysis)
        
        logger.info(f"âœ“ Processed {len(self.analyses)} frames")
        
        # Store in Memgraph
        self.storage_size = self.processor.store_analysis(self.analyses)
        
        # Run queries
        gemini_queries = self._prepare_test_queries()
        gemini_responses = self._run_gemini_queries(gemini_queries)
        
        # Compile results
        result = ValidationResult(
            video_path=self.video_path,
            total_frames=len(frames),
            processed_frames=len(self.analyses),
            total_objects_detected=sum(len(a.objects) for a in self.analyses),
            unique_objects=len(set(obj['class'] for a in self.analyses for obj in a.objects)),
            storage_size_mb=self.storage_size * 0.000001,  # Rough estimate
            queries_tested=gemini_queries,
            gemini_responses=gemini_responses,
            bottlenecks=self._identify_bottlenecks(),
            timestamp=datetime.now().isoformat()
        )
        
        return result
    
    def _load_frames(self) -> List[tuple]:
        """Load frames from video"""
        try:
            cap = cv2.VideoCapture(self.video_path)
            if not cap.isOpened():
                logger.error(f"Cannot open video: {self.video_path}")
                return []
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            frames = []
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append((frame, fps))
                if len(frames) >= self.max_frames:
                    break
            
            cap.release()
            return frames
            
        except Exception as e:
            logger.error(f"Error loading frames: {e}")
            return []
    
    def _prepare_test_queries(self) -> List[Dict[str, str]]:
        """Prepare test queries based on detected objects"""
        queries = []
        
        # Generic queries
        generic_queries = [
            {"question": "What did you see in this video?", "type": "summary"},
            {"question": "How many different rooms did you visit?", "type": "spatial"},
            {"question": "What objects did you interact with?", "type": "objects"},
            {"question": "Describe the temporal sequence of events", "type": "temporal"},
        ]
        queries.extend(generic_queries)
        
        # Object-specific queries
        all_objects = set(obj['class'] for a in self.analyses for obj in a.objects)
        for obj in list(all_objects)[:3]:  # Top 3 objects
            queries.append({
                "question": f"Where did you last see the {obj}?",
                "type": "object_location",
                "object": obj
            })
        
        # Scene queries
        all_scenes = set(a.scene_type for a in self.analyses)
        for scene in all_scenes:
            queries.append({
                "question": f"What did you do in the {scene}?",
                "type": "scene",
                "scene": scene
            })
        
        return queries
    
    def _run_gemini_queries(self, queries: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """Run queries through Gemini"""
        responses = []
        
        try:
            gemini = GeminiQueryInterface()
        except Exception as e:
            logger.warning(f"Gemini interface failed: {e}")
            return []
        
        # Prepare context
        graph_data = self._extract_graph_data()
        descriptions = [a.description for a in self.analyses if a.description]
        timeline = self._build_timeline()
        
        # Query each test
        for query in queries[:5]:  # Limit to 5 queries for testing
            response = gemini.query_with_context(
                question=query['question'],
                graph_data=graph_data,
                frame_descriptions=descriptions,
                scene_timeline=timeline
            )
            responses.append(response)
        
        return responses
    
    def _extract_graph_data(self) -> Dict[str, Any]:
        """Extract observations from analyses for LLM context"""
        observations = []
        for analysis in self.analyses:
            for obj in analysis.objects:
                observations.append({
                    'frame': analysis.frame_idx,
                    'class': obj['class'],
                    'confidence': obj['confidence'],
                    'zone': obj['zone'],
                    'scene_type': analysis.scene_type
                })
        
        return {"observations": observations}
    
    def _build_timeline(self) -> List[Dict[str, Any]]:
        """Build scene change timeline"""
        timeline = []
        prev_scene = None
        
        for analysis in self.analyses:
            if analysis.scene_type != prev_scene:
                timeline.append({
                    'frame': analysis.frame_idx,
                    'timestamp': analysis.timestamp,
                    'scene_type': analysis.scene_type,
                    'objects': [o['class'] for o in analysis.objects]
                })
                prev_scene = analysis.scene_type
        
        return timeline
    
    def _identify_bottlenecks(self) -> List[str]:
        """Identify system bottlenecks"""
        bottlenecks = []
        
        if len(self.analyses) == 0:
            bottlenecks.append("No frames processed - detector not working")
        
        if sum(len(a.objects) for a in self.analyses) == 0:
            bottlenecks.append("No objects detected - YOLO or pipeline issue")
        
        avg_objects_per_frame = sum(len(a.objects) for a in self.analyses) / max(len(self.analyses), 1)
        if avg_objects_per_frame < 1:
            bottlenecks.append("Low object detection rate - check YOLO model")
        
        return bottlenecks
    
    def print_report(self, result: ValidationResult):
        """Print validation report"""
        if not result:
            logger.error("No results to report")
            return
        
        print("\n" + "="*80)
        print("ðŸŽ¯ VALIDATION REPORT")
        print("="*80)
        print(f"\nðŸ“¹ Video: {result.video_path}")
        print(f"â±ï¸  Timestamp: {result.timestamp}")
        print(f"\nðŸ“Š METRICS:")
        print(f"  â€¢ Total frames: {result.total_frames}")
        print(f"  â€¢ Processed: {result.processed_frames}")
        print(f"  â€¢ Objects detected: {result.total_objects_detected}")
        print(f"  â€¢ Unique objects: {result.unique_objects}")
        print(f"  â€¢ Storage size: {result.storage_size_mb:.2f} MB")
        
        if result.bottlenecks:
            print(f"\nâš ï¸  BOTTLENECKS:")
            for bn in result.bottlenecks:
                print(f"  â€¢ {bn}")
        else:
            print(f"\nâœ… No bottlenecks detected!")
        
        if result.gemini_responses:
            print(f"\nðŸ¤– GEMINI RESPONSES ({len(result.gemini_responses)} queries):")
            for i, resp in enumerate(result.gemini_responses, 1):
                if "error" in resp:
                    print(f"\n  [{i}] âŒ Error: {resp['error']}")
                else:
                    print(f"\n  [{i}] Q: {resp.get('question', 'N/A')}")
                    print(f"      A: {resp.get('answer', 'N/A')[:200]}...")
        
        print("\n" + "="*80 + "\n")
        
        # Save full report
        self._save_report(result)
    
    def _save_report(self, result: ValidationResult):
        """Save detailed report to JSON"""
        report_path = Path("validation_report.json")
        with open(report_path, 'w') as f:
            json.dump(asdict(result), f, indent=2, default=str)
        logger.info(f"âœ“ Report saved to {report_path}")


def main():
    parser = argparse.ArgumentParser(description="Validate Orion system end-to-end")
    parser.add_argument("--video", default="data/examples/test.mp4", help="Video file path")
    parser.add_argument("--max-frames", type=int, default=50, help="Max frames to process")
    parser.add_argument("--api-key", help="Gemini API key (or set GOOGLE_API_KEY env var)")
    
    args = parser.parse_args()
    
    # Check video exists
    if not Path(args.video).exists():
        logger.error(f"Video not found: {args.video}")
        sys.exit(1)
    
    # Run validation
    benchmark = ValidationBenchmark(args.video, args.max_frames)
    result = benchmark.run()
    benchmark.print_report(result)


if __name__ == "__main__":
    main()
